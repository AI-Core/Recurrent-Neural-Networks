{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "So far, the neural network architectures we have been using take in a single fixed size input and give a single fixed size output. What if we wanted to model something like language where we want to feed in different length words or sentences? Another issue is that each output is only dependent on the current input. It has no 'memory' of previous inputs so you can't model time dependent variables. Recurrent neural networks address both these issues.\n",
    "\n",
    "They do this by having an internal hidden state which can be thought of as a form of memory. At each time step, the new hidden state is calculated as a function of the previous hidden state and the current input. This hidden state can then be used to represent your output or can be put through another function to compute the outputs. When we say function we are referring to the same one used in standard neural network: linear combination followed by an activation function.\n",
    "\n",
    "### $h_t = f(x_t, h_{t-1})$\n",
    "\n",
    "As shown in the diagram below, which uses a further function to compute the output $o$ from the hidden state $s$, there are three matrices of parameters which we are trying to optimize: U, V and W. The diagram also demonstrates how these networks can be unfolded to show the variables at various time steps.\n",
    "\n",
    "![image](images/rnn.jpg)\n",
    "\n",
    "Standard neural networks can only model one to one relationships while RNNs are extremely flexible in terms of input-output structures which is one of the reasons they are so powerful. You can imagine something like one to many being used to feed in a single image from which a caption is sequentially produced or a many to one being used to feed in a sentence sequentially and give a single output describing the sentiment of the sentence.\n",
    "\n",
    "![image](images/rnnlayouts.jpeg)\n",
    "\n",
    "### Optimization\n",
    "Surprisingly, with this increased complexity in structure, the optimization method does not become any more difficult. Despite having a different name, back-propagation through time, it is essentially the same thing. All you do is feed in your sequence sequentially to get the output, as usual. You then just calculate your error at each timestep and sum it as opposed to calculating the error at a single timestep like standard neural networks. Then you can use gradient descent to update your weights iteratively until you are satisfied with your network's performance.\n",
    "\n",
    "RNNS are generally slower to optimize than standard neural networks as the output at each time step is dependent on the previous output so the operations cannot be parallelized.\n",
    "\n",
    "For a long time it was considered difficult to train RNNs due to two problems called vanishing and exploding gradients. These problems also exist in standard neural network but are greatly emphasized in RNNs. However, modern techniques such as LSTM cells have greatly reduced this difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular task, we will need to do quite a bit of pre-processing. We need to find the number of unique characters in our training text and give each one a unique number so we can one-hot encode them.<br>\n",
    "We start by reading the file, converting all letters to lowercase to reduce the number of characters we need to model, then defining a function which takes in the text and gives up back a dictionary mapping each letter to a unique number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We are going to be implementing a one-to-one character level text prediction model. We will be sequentially feeding in a single character and asking our network to predict the next character as a time dependent function of all the characters that came before it.\n",
    "\n",
    "As always, we begin by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets read in our dataset. We just need a text file which contains the data which we want to model. In this case, I have found a file which contains ~0.5MB of Kendrick Lamar lyrics. You can use a variety of different datasets. There are plenty of which are easily accessible online - check out the links below. Otherwise, is very easy to create your own either by copying and pasting text into a file or creating a bot to automatically do this for you.\n",
    "\n",
    "[Datasets repo 1](https://github.com/cedricdeboom/character-level-rnn-datasets/tree/master/datasets)\n",
    "\n",
    "We now define our dataset class which we can use to read the dataset and use it with a pytorch dataloader for easy sampling. \n",
    "\n",
    "We first open the file and read all the data.\n",
    "\n",
    "Each text character will be represented by a unique number so we first need all the unique characters in our text. Once we have this, we create a dictionary which maps from a unique number to a letter. After defining the reverse mapping aswell, we use the dictionary to convert our original string into a list of numbers where each number represents a text character.\n",
    "\n",
    "The labels are simply the input but shifted by one as we are always predicting the next character based on the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNDataset():\n",
    "    def __init__(self, txt_file_path='Data/lyrics.txt', chunk_size=100, transform=None):\n",
    "        self.txt_file_path = txt_file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        #open our text file and read all the data into the rawtxt variable\n",
    "        with open('lyrics.txt', 'r') as file:\n",
    "            rawtxt = file.read()\n",
    "\n",
    "        #turn all of the text into lowercase as it will reduce the number of characters that our algorithm needs to learn\n",
    "        rawtxt = rawtxt.lower()\n",
    "        \n",
    "        letters = set(rawtxt) #returns the list of unique characters in our raw text\n",
    "        self.nchars = len(letters) #number of unique characters in our text file\n",
    "        self.num_to_let = dict(enumerate(letters)) #created the dictionary mapping\n",
    "        self.let_to_num = dict(zip(self.num_to_let.values(), self.num_to_let.keys())) #create the reverse mapping so we can map from a character to a unique number\n",
    "        \n",
    "        txt = list(rawtxt)#convert string to list\n",
    "        #iterate through our text and change the value for each character to its mapped value\n",
    "        for k, letter in enumerate(txt):\n",
    "            txt[k] = self.let_to_num[letter]\n",
    "\n",
    "        self.X = np.array(txt)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)-1-self.chunk_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx:idx+self.chunk_size]\n",
    "        y = self.X[idx+1:idx+self.chunk_size+1]\n",
    "        \n",
    "        if self.transform:\n",
    "            x, y = self.transform((x, y))\n",
    "    \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "class ToLongTensor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, inp):\n",
    "        return (torch.LongTensor(var) for var in inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First input tensor([50, 15, 18, 53, 44, 38, 44, 52, 18, 51, 18, 51, 10, 18, 52, 44,  6, 15,\n",
      "        52, 54, 26, 44,  6, 50,  5, 12, 21, 38, 25, 27, 18,  6, 44, 50,  5, 12,\n",
      "        44, 25, 52, 38, 51, 18, 44, 50, 46, 46,  3, 21, 50,  5, 25, 18,  6, 55,\n",
      "        34, 38,  5, 18,  6,  6, 18, 44, 50, 44,  5, 38, 16, 16, 50, 44, 21, 38,\n",
      "        19, 27, 44,  6,  3, 51, 18, 44, 25,  3, 54,  5, 19, 18, 52, 34, 18, 38,\n",
      "        19,  6, 55, 10, 54, 19, 44,  5,  3, 21])\n",
      "First label tensor([15, 18, 53, 44, 38, 44, 52, 18, 51, 18, 51, 10, 18, 52, 44,  6, 15, 52,\n",
      "        54, 26, 44,  6, 50,  5, 12, 21, 38, 25, 27, 18,  6, 44, 50,  5, 12, 44,\n",
      "        25, 52, 38, 51, 18, 44, 50, 46, 46,  3, 21, 50,  5, 25, 18,  6, 55, 34,\n",
      "        38,  5, 18,  6,  6, 18, 44, 50, 44,  5, 38, 16, 16, 50, 44, 21, 38, 19,\n",
      "        27, 44,  6,  3, 51, 18, 44, 25,  3, 54,  5, 19, 18, 52, 34, 18, 38, 19,\n",
      "         6, 55, 10, 54, 19, 44,  5,  3, 21, 44]) \n",
      "\n",
      "Number of unique chatacters: 56\n",
      "Length of dataset: 24341\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "chunk_size = 100 #the length of the sequences which we will optimize over\n",
    "\n",
    "train_data = CharRNNDataset('lyrics.txt', chunk_size=100, transform=ToLongTensor())\n",
    "x, y = train_data[0]\n",
    "print('First input', x)\n",
    "print('First label', y, '\\n')\n",
    "\n",
    "nchars = train_data.nchars\n",
    "num_to_let = train_data.num_to_let\n",
    "let_to_num = train_data.let_to_num\n",
    "\n",
    "print('Number of unique chatacters:', nchars)\n",
    "print('Length of dataset:', len(train_data))\n",
    "\n",
    "train_loader = DataLoader(train_data,# make the training dataloader\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model which takes in variables defining its structure as parameters. The encoder converts each unique number into an embedding which is fed into the rnn model. The RNN calculates the hidden state which is converted into an output through a fully connected layer called the decoder.<br>\n",
    "We also define the init_hidden function which outputs us a tensor of zeros of the required size for the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = torch.nn.Embedding(input_size, input_size)\n",
    "        self.i2h = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedding = self.encoder(x) #encode our input into a vector embedding\n",
    "        combined = torch.cat((embedding, hidden), 1)\n",
    "        hidden = torch.tanh(self.i2h(combined))\n",
    "        output = self.i2o(hidden)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        return torch.zeros(x.shape[0], self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate our model, define the appropriate hyper-parameters, cost function and optimizer. We will be training on ranom samples from the text of length chunk_size so it is what batch size is to normal neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-params\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "\n",
    "myrnn = CharRNN(nchars, 512, nchars) #instantiate our model from the class defined earlier\n",
    "criterion = torch.nn.NLLLoss() #define our cost function\n",
    "optimizer = torch.optim.Adam(myrnn.parameters(), lr=lr) #choose optimizer\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training loop, sequentially feeding in multiple batches of random chunks of text, summing the cost for each character in the sequence (backpropagation through time) and calculating the gradients to update our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "def train(model, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0 #stores the cost per epoch\n",
    "        generatedstrings = ['']*batch_size #stores the text generated by our model each epoch\n",
    "        #given our chunk size, how many chunks do we need to optimize over to have gone thorough our whole dataset\n",
    "        for idx, (x, y) in enumerate(train_loader):\n",
    "            loss = 0 #cost for this chunk\n",
    "            h = model.init_hidden(x) #initialize our hidden state to 0s\n",
    "            #sequentially input each character in our sequence and calculate loss\n",
    "            for i in range(chunk_size):\n",
    "                out, h = myrnn.forward(x[:, i], h) #calculate outputs based on input and previous hidden state\n",
    "                \n",
    "                #based on our output, what character does our network predict is next?\n",
    "                #print(out)\n",
    "                _, outl = out.data.max(1)\n",
    "                for k in range(len(outl)):\n",
    "                    letter = num_to_let[outl[k].item()]\n",
    "                    generatedstrings[k]+=letter #add the predicted letter to our generated sequence\n",
    "                loss += criterion(out, y[:, i]) #add the cost for this input to the cost for this current chunk\n",
    "            \n",
    "            writer.add_scalar('Loss/Train', loss/chunk_size, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "            \n",
    "            #based on the sum of the cost for this sequence (backpropagation through time) calculate the gradients and update our weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss+=loss.item() #add the cost of this sequence to the cost of this epoch\n",
    "        epoch_loss /= len(train_loader.dataset) #divide by the number of chunks per epoch to get average cost per epoch\n",
    "\n",
    "        print('Epoch ', epoch+1, ' Avg Loss: ', epoch_loss)\n",
    "        print('Generated text: ', generatedstrings[np.random.randint(0, batch_size)][0:750], '\\n')\n",
    "        \n",
    "train(myrnn, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text above picks the most probable next character each time. This is not the best way to do it as our model will be deterministic so it will produce the same text over and over again. To get it producing different text, we should instead sample from the probability distribution of possible next letters output by the network. That is what we will do with the next function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maparray(txt, mapdict):\n",
    "    txt = list(txt)\n",
    "    #iterate through our text and change the value for each character to its mapped value\n",
    "    for k, letter in enumerate(txt):\n",
    "        txt[k] = mapdict[letter]\n",
    "    txt = np.array(txt)\n",
    "    return txt\n",
    "\n",
    "def generate(model, prime_str='a', str_len=150, temperature=0.75):\n",
    "    generated = prime_str\n",
    "    \n",
    "    prime_str = maparray(prime_str, let_to_num)\n",
    "    x = torch.LongTensor(prime_str).unsqueeze(0)\n",
    "    \n",
    "    #initialize hidden state\n",
    "    h = model.init_hidden(x)\n",
    "    \n",
    "    #primes our hidden state with the input string\n",
    "    for i in range(x.shape[1]):\n",
    "        out, h = model.forward(x[:, i], h)\n",
    "    \n",
    "    x = x[:, -1]\n",
    "    for i in range(str_len):\n",
    "        out, h = myrnn.forward(x, h)\n",
    "        \n",
    "        out_dist = out.data.view(-1).div(temperature).exp()\n",
    "        sample = torch.multinomial(out_dist, 1).item()\n",
    "        pred_char = num_to_let[sample]\n",
    "        \n",
    "        generated += pred_char\n",
    "        \n",
    "        x = torch.LongTensor([sample])\n",
    "    \n",
    "    return generated\n",
    "        \n",
    "gen = generate(myrnn, 'this be ', 2000, 0.75)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        #store input parameters in the object so we can use them later on\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #required functions for model\n",
    "        self.encoder = torch.nn.Embedding(input_size, input_size)\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.decoder = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedding = self.encoder(x.view(-1)) #encode our input into a vector embedding\n",
    "        output, hidden = self.rnn(embedding.view(-1, 1, self.input_size), hidden) #calculate the output from our rnn based on our input and previous hidden state\n",
    "        output = self.decoder(output.view(-1, self.hidden_size)) #calculate our output based on output of rnn\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        return torch.zeros(self.n_layers, x.shape[0], self.hidden_size) #initialize our hidden state to a matrix of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-params\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "\n",
    "myrnn = CharRNN(nchars, 512, nchars) #instantiate our model from the class defined earlier\n",
    "criterion = torch.nn.NLLLoss() #define our cost function\n",
    "optimizer = torch.optim.Adam(myrnn.parameters(), lr=lr) #choose optimizer\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(myrnn, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this be years\n",
      "and forkid.\"t, coll shit without a bid freat spick off me, byouch tas aye\n",
      "you do not amaze me, aye\n",
      "i blew me away, you think more beauty in blue green and grey\n",
      "all my solomon up north, 12 years a slave\n",
      "12 years of age, thinkin’ my shade tho sheterr\n",
      "dount don't want to self-destruct\n",
      "the evils of lucy was all around me\n",
      "so i well of me marain took my stage, i'm the sandman (sandman)\n",
      "get the fuck off my stage, i'm the sandman (sandman)\n",
      "get the fuck off my stage, i'm the sandman (sandman)\n",
      "get the fuck off my stage, i'm the realest nigga, my fiers, new, i got winners on the way\n",
      "you ain't never gotta say shit, uh\n",
      "and i know your taste is, a little bit hmm, high maintenance, uh\n",
      "everybody else basic, you live life on an everyday basis\n",
      "with poetic justice, poetic justice\n",
      "if i told you that a flower bloomed in a dark room, would you trust it?\n",
      "i mean you need to hear this, love is not just a verb\n",
      "it's you looking in the mirror, love is not just a verb\n",
      "it's you looking in the mirror, love is not just a verb\n",
      "it's you looking in the mirror, love is not just a verb\n",
      "it's you looking in the mirror, love is not just a verb\n",
      "it's you looking in the mirror, love is not just a verb\n",
      "it's you looking in the mirror, love is not just a verb\n",
      "it's you looking in the mirror, love is not just a verb\n",
      "it's you looking in the mirror, love is not just a verb\n",
      "it's you looking in the mirr, know just, know just, know just, know just, know just what you want\n",
      "poetic justice, put it in a sook, pound me, \"you remember you was conflicted, misusing your influence\n",
      "\n",
      "[introbe myself, i no longer need cupid\n",
      "and forcin’ my dark side like a young george lucas\n",
      "light don’t mean you smart, bein’ dark don’t make you stupid\n",
      "and frame of mind for them bustas, ain’t talkin’ “woohah!”\n",
      "need a paradox for the pair of dots they tutored\n",
      "like two ties, l-l, you lose two times\n",
      "if you don’t see you beautiful in your complexion\n",
      "it ain’t complex to put it in context\n",
      "find the air beneath the kite, that’s the context\n",
      "yeah, baby\n"
     ]
    }
   ],
   "source": [
    "gen = generate(myrnn, 'this be ', 2000, 0.75)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
