{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "So far, the neural network architectures we have been using take in a single fixed size input and give a single fixed size output. What if we wanted to model something like language where we want to feed in different length words or sentences? Another issue is that each output is only dependent on the current input. It has no 'memory' of previous inputs so you can't model time dependent variables. Recurrent neural networks address both these issues.\n",
    "\n",
    "They do this by having an internal hidden state which can be thought of as a form of memory. At each time step, the new hidden state is calculated as a function of the previous hidden state and the current input. This hidden state can then be used to represent your output or can be put through another function to compute the outputs. When we say function we are referring to the same one used in standard neural network: linear combination followed by an activation function.\n",
    "\n",
    "### $h_t = f(x_t, h_{t-1})$\n",
    "\n",
    "As shown in the diagram below, which uses a further function to compute the output $o$ from the hidden state $s$, there are three matrices of parameters which we are trying to optimize: U, V and W. The diagram also demonstrates how these networks can be unfolded to show the variables at various time steps.\n",
    "\n",
    "![image](images/rnn.jpg)\n",
    "\n",
    "Standard neural networks can only model one to one relationships while RNNs are extremely flexible in terms of input-output structures which is one of the reasons they are so powerful. You can imagine something like one to many being used to feed in a single image from which a caption is sequentially produced or a many to one being used to feed in a sentence sequentially and give a single output describing the sentiment of the sentence.\n",
    "\n",
    "![image](images/rnnlayouts.jpeg)\n",
    "\n",
    "### Optimization\n",
    "Surprisingly, with this increased complexity in structure, the optimization method does not become any more difficult. Despite having a different name, back-propagation through time, it is essentially the same thing. All you do is feed in your sequence sequentially to get the output, as usual. You then just calculate your error at each timestep and sum it as opposed to calculating the error at a single timestep like standard neural networks. Then you can use gradient descent to update your weights iteratively until you are satisfied with your network's performance.\n",
    "\n",
    "RNNS are generally slower to optimize than standard neural networks as the output at each time step is dependent on the previous output so the operations cannot be parallelized.\n",
    "\n",
    "For a long time it was considered difficult to train RNNs due to two problems called vanishing and exploding gradients. These problems also exist in standard neural network but are greatly emphasized in RNNs. However, modern techniques such as LSTM cells have greatly reduced this difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular task, we will need to do quite a bit of pre-processing. We need to find the number of unique characters in our training text and give each one a unique number so we can one-hot encode them.<br>\n",
    "We start by reading the file, converting all letters to lowercase to reduce the number of characters we need to model, then defining a function which takes in the text and gives up back a dictionary mapping each letter to a unique number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We are going to be implementing a one-to-one character level text prediction model. We will be sequentially feeding in a single character and asking our network to predict the next character as a time dependent function of all the characters that came before it.\n",
    "\n",
    "As always, we begin by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets read in our dataset. We just need a text file which contains the data which we want to model. In this case, I have found a file which contains ~0.5MB of Kendrick Lamar lyrics. You can use a variety of different datasets. There are plenty of which are easily accessible online - check out the links below. Otherwise, is very easy to create your own either by copying and pasting text into a file or creating a bot to automatically do this for you.\n",
    "\n",
    "[Datasets repo 1](https://github.com/cedricdeboom/character-level-rnn-datasets/tree/master/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in progress\n",
    "'''class S40dataset():\n",
    "\n",
    "    def __init__(self, txt_file_path='Data/lyrics.txt', chunk_size=100, transform=None):\n",
    "        self.txt_file_path = txt_file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        #open our text file and read all the data into the rawtxt variable\n",
    "        with open('lyrics.txt', 'r') as file:\n",
    "            rawtxt = file.read()\n",
    "\n",
    "        #turn all of the text into lowercase as it will reduce the number of characters that our algorithm needs to learn\n",
    "        rawtxt = rawtxt.lower()\n",
    "        \n",
    "        letters = set(rawtxt) #returns the list of unique characters in our raw text\n",
    "        self.num_to_let = dict(enumerate(letters)) #created the dictionary mapping\n",
    "        self.let_to_num = dict(zip(num_to_let.values(), num_to_let.keys())) #create the reverse mapping so we can map from a character to a unique number\n",
    "        \n",
    "        rawtxt = list(rawtxt)#convert string to list\n",
    "        #iterate through our text and change the value for each character to its mapped value\n",
    "        for k, letter in enumerate(rawtxt):\n",
    "            rawtxt[k] = self.let_to_num[letter]\n",
    "\n",
    "        self.X = np.array(txt)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)-1-self.chunk_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[i:i+self.chunk_size]\n",
    "        y = self.X[i+1:i+self.chunk_size+1]\n",
    "        \n",
    "        if self.transform:\n",
    "            x, y = self.transform(x, y)\n",
    "    \n",
    "        return x, y\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 24442\n",
      "Number of unique characters: 56\n"
     ]
    }
   ],
   "source": [
    "#open our text file and read all the data into the rawtxt variable\n",
    "with open('lyrics.txt', 'r') as file:\n",
    "    rawtxt = file.read()\n",
    "\n",
    "#turn all of the text into lowercase as it will reduce the number of characters that our algorithm needs to learn\n",
    "rawtxt = rawtxt.lower()\n",
    "\n",
    "#returns a dictionary that allows us to map from a unique number to a unique character in our text\n",
    "def create_map(rawtxt):\n",
    "    letters = set(rawtxt) #returns the list of unique characters in our raw text\n",
    "    lettermap = dict(enumerate(letters)) #created the dictionary mapping\n",
    "    return lettermap\n",
    "\n",
    "num_to_let = create_map(rawtxt) #store the dictionary mapping from numbers to characters in a variable\n",
    "let_to_num = dict(zip(num_to_let.values(), num_to_let.keys())) #create the reverse mapping so we can map from a character to a unique number\n",
    "\n",
    "nchars = len(num_to_let) #number of unique characters in our text file\n",
    "print('Size of dataset:', len(rawtxt))\n",
    "print('Number of unique characters:', nchars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function which takes in text and a dictionary and maps each character in the text to the value specified for it in the dictionary. We then use this to map all of our text into the unique numbers for each character so it can be used with our RNN model. The labels are specified as the input but shifted by one time step as the label for each input is the character which comes after it.\n",
    "\n",
    "We also define the function which returns a random chunk from the text which we can use to train out model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24442,)\n",
      "(24442,)\n"
     ]
    }
   ],
   "source": [
    "def maparray(txt, mapdict):\n",
    "    \n",
    "    txt = list(txt)\n",
    "\n",
    "    #iterate through our text and change the value for each character to its mapped value\n",
    "    for k, letter in enumerate(txt):\n",
    "        txt[k] = mapdict[letter]\n",
    "\n",
    "    txt = np.array(txt)\n",
    "    return txt\n",
    "\n",
    "#map our raw text into our input variables using the function defined earlier and passing in the mapping from letters to numbers\n",
    "X = maparray(rawtxt, let_to_num)\n",
    "Y = np.roll(X, -1, axis=0) #our label is the next character so roll shifts our array by one timestep\n",
    "\n",
    "#conver to torch tensors and make row vectors so we can use them in our torch model\n",
    "X = torch.LongTensor(X)[:-1].unsqueeze(dim=1)\n",
    "Y = torch.LongTensor(Y)[:-1].unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return a random batch for training\n",
    "def random_chunk(chunk_size, batch_size):\n",
    "    start_indicies = np.random.randint(0, len(X)-chunk_size, batch_size)\n",
    "    x = torch.cat([X[i:i+chunk_size].unsqueeze(0) for i in start_indicies])\n",
    "    y = torch.cat([Y[i:i+chunk_size].unsqueeze(0) for i in start_indicies])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model which takes in variables defining its structure as parameters. The encoder converts each unique number into an embedding which is fed into the rnn model. The RNN calculates the hidden state which is converted into an output through a fully connected layer called the decoder.<br>\n",
    "We also define the init_hidden function which outputs us a tensor of zeros of the required size for the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        #store input parameters in the object so we can use them later on\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #required functions for model\n",
    "        self.encoder = torch.nn.Embedding(input_size, input_size)\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.decoder = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        x = self.encoder(x.view(-1)) #encode our input into a vector embedding\n",
    "        x, hidden_state = self.rnn(x.view(-1, 1, self.input_size), hidden_state) #calculate the output from our rnn based on our input and previous hidden state\n",
    "        x = self.decoder(x.view(-1, self.hidden_size)) #calculate our output based on output of rnn\n",
    "        return x, hidden_state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size) #initialize our hidden state to a matrix of 0s\n",
    "\n",
    "    \n",
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = torch.nn.Embedding(input_size, input_size)\n",
    "        self.i2h = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.encoder(x.view(-1)) #encode our input into a vector embedding\n",
    "        combined = torch.cat((x, hidden), 1)\n",
    "        hidden = torch.tanh(self.i2h(combined))\n",
    "        output = self.i2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate our model, define the appropriate hyper-parameters, cost function and optimizer. We will be training on ranom samples from the text of length chunk_size so it is what batch size is to normal neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-params\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "chunk_size = 100 #the length of the sequences which we will optimize over\n",
    "batch_size = 32\n",
    "\n",
    "myrnn = CharRNN(nchars, 512, nchars) #instantiate our model from the class defined earlier\n",
    "#criterion = torch.nn.CrossEntropyLoss() #define our cost function\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(myrnn.parameters(), lr=lr) #choose optimizer\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2a10c3351e06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generated text: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneratedstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m750\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-2a10c3351e06>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m#based on the sum of the cost for this sequence (backpropagation through time) calculate the gradients and update our weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0 #stores the cost per epoch\n",
    "        generatedstrings = ['']*batch_size #stores the text generated by our model each epoch\n",
    "        #given our chunk size, how many chunks do we need to optimize over to have gone thorough our whole dataset\n",
    "        for idx in range(len(X)//chunk_size):\n",
    "            loss = 0 #cost for this chunk\n",
    "            x, y = random_chunk(chunk_size, batch_size) #get a random sequence chunk to train\n",
    "            h = myrnn.init_hidden(batch_size) #initialize our hidden state to 0s\n",
    "        #x = F.softmax(x, dim=1)\n",
    "            #sequentially input each character in our sequence and calculate loss\n",
    "            for i in range(chunk_size):\n",
    "                out, h = myrnn.forward(x[:, i, :], h) #calculate outputs based on input and previous hidden state\n",
    "\n",
    "                #based on our output, what character does our network predict is next?\n",
    "                _, outl = out.data.max(1)\n",
    "                for k in range(batch_size):\n",
    "                    letter = num_to_let[outl[k].item()]\n",
    "                    generatedstrings[k]+=letter #add the predicted letter to our generated sequence\n",
    "                loss += criterion(out, y[:, i, :].flatten()) #add the cost for this input to the cost for this current chunk\n",
    "            \n",
    "            writer.add_scalar('Loss/Train', loss/chunk_size, epoch*(len(X)//chunk_size) + idx)    # write loss to a graph\n",
    "            \n",
    "            #based on the sum of the cost for this sequence (backpropagation through time) calculate the gradients and update our weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss+=loss.item() #add the cost of this sequence to the cost of this epoch\n",
    "        total_loss /= len(X) #divide by the number of chunks per epoch to get average cost per epoch\n",
    "\n",
    "        print('Epoch ', epoch+1, ' Avg Loss: ', total_loss)\n",
    "        print('Generated text: ', generatedstrings[np.random.randint(0, batch_size)][0:750], '\\n')\n",
    "        \n",
    "train(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the axes for plotting our cost per epoch. Define the training loop, sequentially feeding in a random chunk of text, summing the cost for each character in the sequence (backpropagation through time) and calculating the gradients to update our weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text above picks the most probable next character each time. This is not the best way to do it as our model will be deterministic so it will produce the same text over and over again. To get it producing different text, we should instead sample from the probability distribution of possible next letters output by the network. That is what we will do with the next function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this be out the like you stupid\n",
      "and frame of mind for them bustas, ain’t talkin’ “woohah!”\n",
      "need a paradox for the pair of dots they tutored\n",
      "like two ties, l-l, you lose two times\n",
      "if you don’t see you beautiful in ye\n",
      "in' on your baby killed by a coward\n",
      "i can't even keep the peace, dompthe, \"hound funds\n",
      "he was sobbin', he was mobbin', way belligerent and drunk\n",
      "talkin' out his head, philosphin' on what the lord had done\n",
      "he said: \"k-dot, can you pray for me?\n",
      "it's been a fucked up day for me\n",
      "i know that you anointed, show me how to overcome.\"\n",
      "he was lookin' for some closure\n",
      "hopin' i could bring him closer\n",
      "to the spiritual, my spirit do no better, but i told him\n",
      "\"i can't sugarcoat the answer for you, this is how i feel:\n",
      "if somebody kill my son, that mean somebody gettin' killed.\"\n",
      "tell me what you do for love, loyalty, and passion of\n",
      "all the memories collected, moments you could never touch\n",
      "\n",
      "i'll wait in front a niggas spot and watch him hit his lloke)\n",
      "wi got the whole world talkin' (king kunta)\n",
      "everybody wanna cut the legs off him\n",
      "\n",
      "when you got the yams (what's the yams?)\n",
      "the yam brought it out of richard pryor\n",
      "manipulated bill clinton with desires\n",
      "24/7, 365 days times two\n",
      "i was contemplatin' gettin' on stage\n",
      "just to go back to the hood see my enemies and say...\n",
      "\n",
      "bitch where you when i was walkin'?\n",
      "now i run the game got the whole world talkin' (king kunta)\n",
      "everybody wanna cut the legs off him\n",
      "\n",
      "(funk, funk, funk, funk, funk, funk)\n",
      "we want the funk\n",
      "we want the funk\n",
      "(do you want the funk?)\n",
      "we want the funk\n",
      "i remember you was conflicted\n",
      "misusing your influence\n",
      "sometimes i did the same\n",
      "abusing my power full of resentment\n",
      "resentment that turned into a deep depression\n",
      "found myself screaming in a hotel room\n",
      "i didn't want to self-destruct\n",
      "the evils of lusy framey\n",
      "wang messers sick a fuck about your complexion, i know what the germans done\n",
      "sneak (dissin’)\n",
      "sneak mesthite wall me ones\n",
      "we want the funk\n",
      "(do you want the funk?)\n",
      "we want the funk\n",
      "(do you want the funk?)\n",
      "we want the funk\n",
      "(do you \n"
     ]
    }
   ],
   "source": [
    "def generate(prime_str='a', str_len=150, temperature=0.75):\n",
    "    generated = prime_str\n",
    "    \n",
    "    #initialize hidden state\n",
    "    h = myrnn.init_hidden(1)\n",
    "    \n",
    "    prime_str = maparray(prime_str, let_to_num)\n",
    "    x = torch.LongTensor(prime_str)\n",
    "    \n",
    "    #primes our hidden state with the input string\n",
    "    for i in range(len(x)):\n",
    "        out, h = myrnn.forward(x[i].unsqueeze(0), h)\n",
    "    \n",
    "    x = x[-1]\n",
    "    \n",
    "    for i in range(str_len):\n",
    "        out, h = myrnn.forward(x, h)\n",
    "        \n",
    "        out_dist = out.data.view(-1).div(temperature).exp()\n",
    "        sample = torch.multinomial(out_dist, 1).item()\n",
    "        pred_char = num_to_let[sample]\n",
    "        \n",
    "        generated += pred_char\n",
    "        \n",
    "        x = torch.LongTensor([sample])\n",
    "    \n",
    "    return generated\n",
    "        \n",
    "    \n",
    "\n",
    "gen = generate('this be ', 2000, 0.75)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
