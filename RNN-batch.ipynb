{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "So far, the neural network architectures we have been using take in a single fixed size input and give a single fixed size output. What if we wanted to model something like language where we want to feed in different length words or sentences? Another issue is that each output is only dependent on the current input. It has no 'memory' of previous inputs so you can't model time dependent variables. Recurrent neural networks address both these issues.\n",
    "\n",
    "They do this by having an internal hidden state which can be thought of as a form of memory. At each time step, the new hidden state (h) is calculated as a function of the previous hidden state and the current input (x). This hidden state can then be used to represent your output or can be put through another transformation to compute the outputs (y).\n",
    "\n",
    "The left diagram below shows how we represent a RNN whereas the right one shows a RNN which has been \"unrolled\" over time so we see the value of each variable at successive time steps.\n",
    "\n",
    "![image](images/RNN_basic.JPG)\n",
    "\n",
    "The diagram below shows the actual matrix representation of each of the variables in the RNN. Instead of doing two separate matrix multiplications on the input and previous hidden state to calculate the next hidden state, we can concatenate those two variables into  single I vector and W matrix\n",
    "\n",
    "![image](images/RNN_matrices.JPG)\n",
    "\n",
    "There is also an alternative way of visually representing the RNN which lets us see how similar it is to a fully connected network.\n",
    "\n",
    "![image](images/RNN_other_rep.JPG)\n",
    "\n",
    "Standard neural networks can only model one to one relationships while RNNs are extremely flexible in terms of input-output structures which is one of the reasons they are so powerful. You can imagine something like one to many being used to feed in a single image from which a caption is sequentially produced or a many to one being used to feed in a sentence sequentially and give a single output describing the sentiment of the sentence.\n",
    "\n",
    "![image](images/RNN_layouts.JPG)\n",
    "Source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "### Optimization\n",
    "Surprisingly, with this increased complexity in structure, the optimization method does not become any more difficult. Despite having a different name, back-propagation through time, it is essentially the same thing. All you do is feed in your sequence sequentially to get the output, as usual. You then just calculate your error at each timestep and sum it as opposed to calculating the error at a single timestep like standard neural networks. Then you can use gradient descent to update your weights iteratively until you are satisfied with your network's performance.\n",
    "\n",
    "![image](images/RNN_BPTT.JPG)\n",
    "\n",
    "RNNS are generally slower to optimize than standard neural networks as the output at each time step is dependent on the previous output so the operations cannot be parallelized.\n",
    "![image](images/RNN_parallel.JPG)\n",
    "\n",
    "For a long time it was considered difficult to train RNNs due to two problems called vanishing and exploding gradients. These problems also exist in standard neural network but are greatly emphasized in RNNs. However, modern techniques such as LSTM cells have greatly reduced this difficulty.\n",
    "![image](images/RNN_gradient.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We are going to be implementing a one-to-one character level text prediction model. We will be sequentially feeding in a single character and asking our network to predict the next character as a time dependent function of all the characters that came before it.\n",
    "\n",
    "As always, we begin by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets read in our dataset. We just need a text file which contains the data which we want to model. In this case, I have found a file which contains ~0.5MB of Kendrick Lamar lyrics. You can use a variety of different datasets. There are plenty of which are easily accessible online - check out the links below. Otherwise, is very easy to create your own either by copying and pasting text into a file or creating a bot to automatically do this for you.\n",
    "\n",
    "[Datasets repo 1](https://github.com/cedricdeboom/character-level-rnn-datasets/tree/master/datasets)\n",
    "\n",
    "We now define our dataset class which we can use to read the dataset and use it with a pytorch dataloader for easy sampling. \n",
    "\n",
    "We first open the file and read all the data.\n",
    "\n",
    "Each text character will be represented by a unique number so we first need all the unique characters in our text. Once we have this, we create a dictionary which maps from a unique number to a letter. After defining the reverse mapping aswell, we use the dictionary to convert our original string into a list of numbers where each number represents a text character.\n",
    "\n",
    "The labels are simply the input but shifted by one as we are always predicting the next character based on the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNDataset():\n",
    "    def __init__(self, txt_file_path='Data/lyrics.txt', chunk_size=100, transform=None):\n",
    "        self.txt_file_path = txt_file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        #open our text file and read all the data into the rawtxt variable\n",
    "        with open('lyrics.txt', 'r') as file:\n",
    "            rawtxt = file.read()\n",
    "\n",
    "        #turn all of the text into lowercase as it will reduce the number of characters that our algorithm needs to learn\n",
    "        rawtxt = rawtxt.lower()\n",
    "        \n",
    "        letters = set(rawtxt) #returns the list of unique characters in our raw text\n",
    "        self.nchars = len(letters) #number of unique characters in our text file\n",
    "        self.num_to_let = dict(enumerate(letters)) #created the dictionary mapping\n",
    "        self.let_to_num = dict(zip(self.num_to_let.values(), self.num_to_let.keys())) #create the reverse mapping so we can map from a character to a unique number\n",
    "        \n",
    "        txt = list(rawtxt)#convert string to list\n",
    "        #iterate through our text and change the value for each character to its mapped value\n",
    "        for k, letter in enumerate(txt):\n",
    "            txt[k] = self.let_to_num[letter]\n",
    "\n",
    "        self.X = np.array(txt)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)-1-self.chunk_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx:idx+self.chunk_size]\n",
    "        y = self.X[idx+1:idx+self.chunk_size+1]\n",
    "        \n",
    "        if self.transform:\n",
    "            x, y = self.transform((x, y))\n",
    "    \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "class ToLongTensor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, inp):\n",
    "        return (torch.LongTensor(var) for var in inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First input tensor([50, 15, 18, 53, 44, 38, 44, 52, 18, 51, 18, 51, 10, 18, 52, 44,  6, 15,\n",
      "        52, 54, 26, 44,  6, 50,  5, 12, 21, 38, 25, 27, 18,  6, 44, 50,  5, 12,\n",
      "        44, 25, 52, 38, 51, 18, 44, 50, 46, 46,  3, 21, 50,  5, 25, 18,  6, 55,\n",
      "        34, 38,  5, 18,  6,  6, 18, 44, 50, 44,  5, 38, 16, 16, 50, 44, 21, 38,\n",
      "        19, 27, 44,  6,  3, 51, 18, 44, 25,  3, 54,  5, 19, 18, 52, 34, 18, 38,\n",
      "        19,  6, 55, 10, 54, 19, 44,  5,  3, 21])\n",
      "First label tensor([15, 18, 53, 44, 38, 44, 52, 18, 51, 18, 51, 10, 18, 52, 44,  6, 15, 52,\n",
      "        54, 26, 44,  6, 50,  5, 12, 21, 38, 25, 27, 18,  6, 44, 50,  5, 12, 44,\n",
      "        25, 52, 38, 51, 18, 44, 50, 46, 46,  3, 21, 50,  5, 25, 18,  6, 55, 34,\n",
      "        38,  5, 18,  6,  6, 18, 44, 50, 44,  5, 38, 16, 16, 50, 44, 21, 38, 19,\n",
      "        27, 44,  6,  3, 51, 18, 44, 25,  3, 54,  5, 19, 18, 52, 34, 18, 38, 19,\n",
      "         6, 55, 10, 54, 19, 44,  5,  3, 21, 44]) \n",
      "\n",
      "Number of unique chatacters: 56\n",
      "Length of dataset: 24341\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "chunk_size = 100 #the length of the sequences which we will optimize over\n",
    "\n",
    "train_data = CharRNNDataset('lyrics2.txt', chunk_size=100, transform=ToLongTensor())\n",
    "x, y = train_data[0]\n",
    "print('First input', x)\n",
    "print('First label', y, '\\n')\n",
    "\n",
    "nchars = train_data.nchars\n",
    "num_to_let = train_data.num_to_let\n",
    "let_to_num = train_data.let_to_num\n",
    "\n",
    "print('Number of unique chatacters:', nchars)\n",
    "print('Length of dataset:', len(train_data))\n",
    "\n",
    "train_loader = DataLoader(train_data,# make the training dataloader\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model which takes in variables defining its structure as parameters. The encoder converts each unique number into an embedding which is fed into the rnn model. The RNN calculates the hidden state which is converted into an output through a fully connected layer called the decoder.<br>\n",
    "We also define the init_hidden function which outputs us a tensor of zeros of the required size for the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = torch.nn.Embedding(input_size, input_size)\n",
    "        self.i2h = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedding = self.encoder(x) #encode our input into a vector embedding\n",
    "        combined = torch.cat((embedding, hidden), 1)\n",
    "        hidden = torch.tanh(self.i2h(combined))\n",
    "        output = self.h2o(hidden)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        return torch.zeros(x.shape[0], self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate our model, define the appropriate hyper-parameters, cost function and optimizer. We will be training on ranom samples from the text of length chunk_size so it is what batch size is to normal neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-params\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "\n",
    "myrnn = CharRNN(nchars, 512, nchars) #instantiate our model from the class defined earlier\n",
    "criterion = torch.nn.NLLLoss() #define our cost function\n",
    "optimizer = torch.optim.Adam(myrnn.parameters(), lr=lr) #choose optimizer\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training loop, sequentially feeding in multiple batches of random chunks of text, summing the cost for each character in the sequence (backpropagation through time) and calculating the gradients to update our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "def train(model, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0 #stores the cost per epoch\n",
    "        generatedstrings = ['']*batch_size #stores the text generated by our model each epoch\n",
    "        #given our chunk size, how many chunks do we need to optimize over to have gone thorough our whole dataset\n",
    "        for idx, (x, y) in enumerate(train_loader):\n",
    "            loss = 0 #cost for this chunk\n",
    "            h = model.init_hidden(x) #initialize our hidden state to 0s\n",
    "            #sequentially input each character in our sequence and calculate loss\n",
    "            for i in range(chunk_size):\n",
    "                out, h = myrnn.forward(x[:, i], h) #calculate outputs based on input and previous hidden state\n",
    "                \n",
    "                #based on our output, what character does our network predict is next?\n",
    "                #print(out)\n",
    "                _, outl = out.data.max(1)\n",
    "                for k in range(len(outl)):\n",
    "                    letter = num_to_let[outl[k].item()]\n",
    "                    generatedstrings[k]+=letter #add the predicted letter to our generated sequence\n",
    "                loss += criterion(out, y[:, i]) #add the cost for this input to the cost for this current chunk\n",
    "            \n",
    "            writer.add_scalar('Loss/Train', loss/chunk_size, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "            \n",
    "            #based on the sum of the cost for this sequence (backpropagation through time) calculate the gradients and update our weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss+=loss.item() #add the cost of this sequence to the cost of this epoch\n",
    "        epoch_loss /= len(train_loader.dataset) #divide by the number of chunks per epoch to get average cost per epoch\n",
    "\n",
    "        print('Epoch ', epoch+1, ' Avg Loss: ', epoch_loss)\n",
    "        print('Generated text: ', generatedstrings[np.random.randint(0, batch_size)][0:750], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(myrnn, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text above picks the most probable next character each time. This is not the best way to do it as our model will be deterministic so it will produce the same text over and over again. To get it producing different text, we should instead sample from the probability distribution of possible next letters output by the network. That is what we will do with the next function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maparray(txt, mapdict):\n",
    "    txt = list(txt)\n",
    "    #iterate through our text and change the value for each character to its mapped value\n",
    "    for k, letter in enumerate(txt):\n",
    "        txt[k] = mapdict[letter]\n",
    "    txt = np.array(txt)\n",
    "    return txt\n",
    "\n",
    "def generate(model, prime_str='a', str_len=150, temperature=0.75):\n",
    "    generated = prime_str\n",
    "    \n",
    "    prime_str = maparray(prime_str, let_to_num)\n",
    "    x = torch.LongTensor(prime_str).unsqueeze(0)\n",
    "    \n",
    "    #initialize hidden state\n",
    "    h = model.init_hidden(x)\n",
    "    \n",
    "    #primes our hidden state with the input string\n",
    "    for i in range(x.shape[1]):\n",
    "        out, h = model.forward(x[:, i], h)\n",
    "    \n",
    "    x = x[:, -1]\n",
    "    for i in range(str_len):\n",
    "        out, h = myrnn.forward(x, h)\n",
    "        \n",
    "        out_dist = out.data.view(-1).div(temperature).exp()\n",
    "        sample = torch.multinomial(out_dist, 1).item()\n",
    "        pred_char = num_to_let[sample]\n",
    "        \n",
    "        generated += pred_char\n",
    "        \n",
    "        x = torch.LongTensor([sample])\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generate(myrnn, 'this be ', 2000, 0.75)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTorch's built in RNN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        #store input parameters in the object so we can use them later on\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #required functions for model\n",
    "        self.encoder = torch.nn.Embedding(input_size, input_size)\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.decoder = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedding = self.encoder(x.view(-1)) #encode our input into a vector embedding\n",
    "        output, hidden = self.rnn(embedding.view(-1, 1, self.input_size), hidden) #calculate the output from our rnn based on our input and previous hidden state\n",
    "        output = self.decoder(output.view(-1, self.hidden_size)) #calculate our output based on output of rnn\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        return torch.zeros(self.n_layers, x.shape[0], self.hidden_size) #initialize our hidden state to a matrix of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-params\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "\n",
    "myrnn = CharRNN(nchars, 512, nchars) #instantiate our model from the class defined earlier\n",
    "criterion = torch.nn.NLLLoss() #define our cost function\n",
    "optimizer = torch.optim.Adam(myrnn.parameters(), lr=lr) #choose optimizer\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(myrnn, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generate(myrnn, 'this be ', 2000, 0.75)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM in PyTorch\n",
    "\n",
    "Only two things change from the above example to use an LSTM instead. Firstly, use torch.nn.LSTM instead of torch.nn.RNN when defining our model. Secondly, we change the init_hidden function so it return an extra matrix of 0s as the LSTM not only has a hidden state but also a cell state which needs to be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        #store input parameters in the object so we can use them later on\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #required functions for model\n",
    "        self.encoder = torch.nn.Embedding(input_size, input_size)\n",
    "        self.rnn = torch.nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.decoder = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedding = self.encoder(x.view(-1)) #encode our input into a vector embedding\n",
    "        output, hidden = self.rnn(embedding.view(-1, 1, self.input_size), hidden) #calculate the output from our rnn based on our input and previous hidden state\n",
    "        output = self.decoder(output.view(-1, self.hidden_size)) #calculate our output based on output of rnn\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        return (torch.zeros(self.n_layers, x.shape[0], self.hidden_size),\n",
    "                torch.zeros(self.n_layers, x.shape[0], self.hidden_size)) #initialize our hidden and cell state to a matrix of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-params\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "\n",
    "myrnn = CharLSTM(nchars, 512, nchars) #instantiate our model from the class defined earlier\n",
    "criterion = torch.nn.NLLLoss() #define our cost function\n",
    "optimizer = torch.optim.Adam(myrnn.parameters(), lr=lr) #choose optimizer\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Avg Loss:  3.936492918511404\n",
      "Generated text:  6w6//s/w;rwwwwww/wwwwwggwgo6w(666gw6psw//w69sw//r///“wb//www666gws(w/(wgwwrgowrgw6gwwwww/rw6nbw6gwgwwwrww/     wowws wwwo//w gohw rrrr wow  wo w 6wb  /owwuwwwo /on   wowwuwwwwoo  w roo/o gwo w 6wb  /ou  r w      w    w         oo   o   w     ow   g w                 u        u                    wouh w                                              w uu                                               n                                                                                                                                                                                                                                                                                                           n                                                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(myrnn, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this be userick\n",
      "show me something natural like ass with some stretch marks\n",
      "still will take you down right on your mamma's couch in polo socks, aye\n",
      "this shit way too crazy, aye\n",
      "you do not amaze me, aye\n",
      "i blew cool from ac, aye\n",
      "obama just paged me, aye\n",
      "i don't fabricate it, aye\n",
      "most of ya'll be faking, aye\n",
      "i stay modest bout it, aye\n",
      "she elaborate it, aye\n",
      "this that grey poupon, that evian, that ted talk, aye\n",
      "watch my soul speak, you let the meds talk, aye\n",
      "if i kill a nigga it won't be the alcohol, aye\n",
      "i'm the realest nigga after all, bitch be humble\n",
      "\n",
      "(hol' up bitch) sit down,\n",
      "(hol' up lil bitch, hol' up, lil bitch) be humble\n",
      "(hol' up bitch) sit down (hol' up sit down lil bitch)\n",
      "(sit down lil bitch) be humble\n",
      "(hol' up, hol' up, hol' up, hol' up lil bitch) sit down\n",
      "(hol' up lil bitch) be humble\n",
      "(hol' up bitch) sit down\n",
      "(hol' up, hol' up, hol' up, hol' up) be humble\n",
      "(hol' up, hol' up, hol' up, hol' up lil bitch) bitch, sit down\n",
      "(hol' up lil bitch) be humble\n",
      "(hol' up bitch) sit down\n",
      "(hol' up, hol' up, hol' up, hol' up) be humble\n",
      "(hol' up, hol' up, hol' up, hol' up lil bitch) sit down\n",
      "(hol' up lil bitch) be humble\n",
      "(hol' up bitch) sit down\n",
      "(hol' up, hol' up, hol' up, hol' up) be humble\n",
      "(hol' up, hol' up, hol' up, hol' up lil bitch) sit down\n",
      "(hol' up lil bitch) be humble\n",
      "(hol' up bitch) sit down\n",
      "(hol' up, hol' up, hol' up, hol' up) be humble\n",
      "(hol' up, hol' up, hol' up, hol' up lil bitch) bitch, sit down\n",
      "(hol' up lil bitch) be humble\n",
      "(hol' up bitch) sit down\n",
      "(hol' up, hol' up, hol' up, hol' up)\n",
      "\n",
      "who dat nigga thinking that he fronting on man man (man man)\n",
      "get the fuck off my stage, i'm the sandman (sandman)\n",
      "get the fuck off my dick, that ain't right\n",
      "i make a play fucking up your whole life\n",
      "i'm so fucking sick and tired of the photoshop\n",
      "show me something natural like afro on richard pryor\n",
      "show me something natural like ass with some stretch marks\n",
      "sindon, they bet me talk my stu scott, ‘scuse me on my 2pac\n",
      "keep your head up, when did you stop? love and die\n",
      "color of your skin, color of y\n"
     ]
    }
   ],
   "source": [
    "gen = generate(myrnn, 'this be ', 2000, 0.75)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
