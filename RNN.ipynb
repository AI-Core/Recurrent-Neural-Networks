{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "So far, the neural network architectures we have been using take in a single fixed size input and give a single fixed size output. What if we wanted to model something like language where we want to feed in different length words or sentences? Another issue is that each output is only dependent on the current input. It has no 'memory' of previous inputs so you can't model time dependent variables. Recurrent neural networks address both these issues.\n",
    "\n",
    "They do this by having an internal hidden state which can be thought of as a form of memory. At each time step, the new hidden state is calculated as a function of the previous hidden state and the current input. This hidden state can then be used to represent your output or can be put through another function to compute the outputs. When we say function we are referring to the same one used in standard neural network: linear combination followed by an activation function.\n",
    "\n",
    "### $h_t = f(x_t, h_{t-1})$\n",
    "\n",
    "As shown in the diagram below, which uses a further function to compute the output $o$ from the hidden state $s$, there are three matrices of parameters which we are trying to optimize: U, V and W. The diagram also demonstrates how these networks can be unfolded to show the variables at various time steps.\n",
    "\n",
    "![image](images/rnn.jpg)\n",
    "\n",
    "Standard neural networks can only model one to one relationships while RNNs are extremely flexible in terms of input-output structures which is one of the reasons they are so powerful. You can imagine something like one to many being used to feed in a single image from which a caption is sequentially produced or a many to one being used to feed in a sentence sequentially and give a single output describing the sentiment of the sentence.\n",
    "\n",
    "![image](images/rnnlayouts.jpeg)\n",
    "\n",
    "### Optimization\n",
    "Surprisingly, with this increased complexity in structure, the optimization method does not become any more difficult. Despite having a different name, back-propagation through time, it is essentially the same thing. All you do is feed in your sequence sequentially to get the output, as usual. You then just calculate your error at each timestep and sum it as opposed to calculating the error at a single timestep like standard neural networks. Then you can use gradient descent to update your weights iteratively until you are satisfied with your network's performance.\n",
    "\n",
    "RNNS are generally slower to optimize than standard neural networks as the output at each time step is dependent on the previous output so the operations cannot be parallelized.\n",
    "\n",
    "For a long time it was considered difficult to train RNNs due to two problems called vanishing and exploding gradients. These problems also exist in standard neural network but are greatly emphasized in RNNs. However, modern techniques such as LSTM cells have greatly reduced this difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular task, we will need to do quite a bit of pre-processing. We need to find the number of unique characters in our training text and give each one a unique number so we can one-hot encode them.<br>\n",
    "We start by reading the file, converting all letters to lowercase to reduce the number of characters we need to model, then defining a function which takes in the text and gives up back a dictionary mapping each letter to a unique number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We are going to be implementing a one-to-one character level text prediction model. We will be sequentially feeding in a single character and asking our network to predict the next character as a time dependent function of all the characters that came before it.\n",
    "\n",
    "As always, we begin by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets read in our dataset. We just need a text file which contains the data which we want to model. In this case, I have found a file which contains ~0.5MB of Kendrick Lamar lyrics. You can use a variety of different datasets. There are plenty of which are easily accessible online - check out the links below. Otherwise, is very easy to create your own either by copying and pasting text into a file or creating a bot to automatically do this for you.\n",
    "\n",
    "[Datasets repo 1](https://github.com/cedricdeboom/character-level-rnn-datasets/tree/master/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 24442\n",
      "Number of unique characters: 56\n"
     ]
    }
   ],
   "source": [
    "#open our text file and read all the data into the rawtxt variable\n",
    "with open('lyrics.txt', 'r') as file:\n",
    "    rawtxt = file.read()\n",
    "\n",
    "#turn all of the text into lowercase as it will reduce the number of characters that our algorithm needs to learn\n",
    "rawtxt = rawtxt.lower()\n",
    "\n",
    "#returns a dictionary that allows us to map from a unique number to a unique character in our text\n",
    "def create_map(rawtxt):\n",
    "    letters = set(rawtxt) #returns the list of unique characters in our raw text\n",
    "    lettermap = dict(enumerate(letters)) #created the dictionary mapping\n",
    "    return lettermap\n",
    "\n",
    "num_to_let = create_map(rawtxt) #store the dictionary mapping from numbers to characters in a variable\n",
    "let_to_num = dict(zip(num_to_let.values(), num_to_let.keys())) #create the reverse mapping so we can map from a character to a unique number\n",
    "\n",
    "nchars = len(num_to_let) #number of unique characters in our text file\n",
    "print('Size of dataset:', len(rawtxt))\n",
    "print('Number of unique characters:', nchars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function which takes in text and a dictionary and maps each character in the text to the value specified for it in the dictionary. We then use this to map all of our text into the unique numbers for each character so it can be used with our RNN model. The labels are specified as the input but shifted by one time step as the label for each input is the character which comes after it.\n",
    "\n",
    "We also define the function which returns a random chunk from the text which we can use to train out model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maparray(txt, mapdict):\n",
    "    \n",
    "    txt = list(txt)\n",
    "\n",
    "    #iterate through our text and change the value for each character to its mapped value\n",
    "    for k, letter in enumerate(txt):\n",
    "        txt[k] = mapdict[letter]\n",
    "\n",
    "    txt = np.array(txt)\n",
    "    return txt\n",
    "\n",
    "#map our raw text into our input variables using the function defined earlier and passing in the mapping from letters to numbers\n",
    "X = maparray(rawtxt, let_to_num)\n",
    "Y = np.roll(X, -1, axis=0) #our label is the next character so roll shifts our array by one timestep\n",
    "\n",
    "#conver to torch tensors and make row vectors so we can use them in our torch model\n",
    "X = torch.LongTensor(X).unsqueeze(dim=1)\n",
    "Y = torch.LongTensor(Y).unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return a random batch for training\n",
    "def random_chunk(chunk_size):\n",
    "    k = np.random.randint(0, len(X)-chunk_size)\n",
    "    return X[k:k+chunk_size], Y[k:k+chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1]), torch.Size([10, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = random_chunk(10)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model which takes in variables defining its structure as parameters. The encoder converts each unique number into an embedding which is fed into the rnn model. The RNN calculates the hidden state which is converted into an output through a fully connected layer called the decoder.<br>\n",
    "We also define the init_hidden function which outputs us a tensor of zeros of the required size for the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChaRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = torch.nn.Embedding(input_size, input_size)\n",
    "        self.i2h = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.encoder(x.view(-1)) #encode our input into a vector embedding\n",
    "        combined = torch.cat((x, hidden), 1)\n",
    "        hidden = torch.tanh(self.i2h(combined))\n",
    "        output = self.i2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "class ChaRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        #store input parameters in the object so we can use them later on\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #required functions for model\n",
    "        self.encoder = torch.nn.Embedding(input_size, input_size)\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.decoder = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        x = self.encoder(x.view(-1, 1)) #encode our input into a vector embedding\n",
    "        x, hidden_state = self.rnn(x.view(1, 1, self.input_size), hidden_state) #calculate the output from our rnn based on our input and previous hidden state\n",
    "        x = self.decoder(x.view(1, -1)) #calculate our output based on output of rnn\n",
    "        return x, hidden_state\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_size) #initialize our hidden state to a matrix of 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate our model, define the appropriate hyper-parameters, cost function and optimizer. We will be training on ranom samples from the text of length chunk_size so it is what batch size is to normal neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-params\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "chunk_size = 100 #the length of the sequences which we will optimize over\n",
    "\n",
    "myrnn = ChaRNN(nchars, 512, nchars) #instantiate our model from the class defined earlier\n",
    "criterion = torch.nn.CrossEntropyLoss() #define our cost function\n",
    "optimizer = torch.optim.Adam(myrnn.parameters(), lr=lr) #choose optimizer\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the axes for plotting our cost per epoch. Define the training loop, sequentially feeding in a random chunk of text, summing the cost for each character in the sequence (backpropagation through time) and calculating the gradients to update our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Avg Loss:  2.4635538214035915\n",
      "Generated text:  ’5/7f7p.ydx]dy]x0y')x7y[dumf’:d9x”di]uo’70d’5/:9ubd 1ś7/y’cyd).\" yx”du7]wuy/y’o)p 9’m0)/:“70j/:q5xd\"\"bf/ don fa dopdoo)z)0 cdondo,\"dod nddoeb)do 0do )donpdo oydo wyd cznzydo]do yxedon fonxdonqn onx dofion  do ndon d os do kdo  ng ands odo oy s do ndon dodoongdondo ndo n on d do  do nc  ofyan  dn adodo  o d t  don  do    o   neo on on   don    do n     do don  n  don    do o ndos odon     do   do   c  o   o     o    o   o         o  o    o     o     o  o    o          do   o    o      on    o    do   o            o       o  o  o  o       o  o    o   o        o     o         o  o    o           aou     o          o     o    o   o            o   o    o    o   o    o   o   o  o              o   do oo o   o oo o   o oo o   o oo o  oo  o   o  o   \n",
      "\n",
      "Epoch  2  Avg Loss:  2.054762767463686\n",
      "Generated text:  ghdhdow  wol' up,litebitch)\n",
      "(e wotele\n",
      "thotch)\n",
      "(hol' up, hol' up, hol' up, hol' up,\n",
      "hotch)ltthdown (htgot aoale   i wnwust wonl teot hoa st wt   atdweouoa\n",
      "d wot wele sud\n",
      " wnwot woth  ttott  t' tn weouols tro tlhal te tou  aile tomd  hostaoo  eohot  tg  [eo reoe theow  ethe wotk ahtg r\n",
      " wne traotn aototeskeahrt\n",
      " the womd wn lhe womd eng   whe woal mhet   wudw leeeti[tgot wnwoot wh titk atgol'  ahld wotn tot   tot   s gal  gndui    tooetou wut  ctou   'wust whlthet' chth tou sou wiow jns totthe fot wy tn te d ,lk\n",
      "e   tou  to l tn, hoo  got tou wiel  ti e tnd tot sonl  tn tou  tieng goll  tn tou  dt knmot the fi l    dne     n (iltn    dil dile  en' ghet   lelc  lnd to th h  suo tis ene  \n",
      "u aeldwot dou  ties gn   lh   me tou  ti e tond  losti o \n",
      "\n",
      "Epoch  3  Avg Loss:  1.8521604493483517\n",
      "Generated text:  ich  y[hua   aow't thlna cutth thaern mowtont towtontaume   ailn toun  w n’t tounltowtontakowtont ahnl tou cotk nball sh   hin   yhrt  won a chlkimllut tot aau ainlbhooc aon ia  iol  ea[hou' ]\n",
      "i  s aehh  bit down (hol' up, hol' up, hol' up, hol' up, hi comble\n",
      "(hol' up, hol' up, hol' up, hol' up,lit n aoo bo  \n",
      "ths to   t nuck   tn ton aoo ma bngiow jhe  sou wnd n'    iheu io non ih tne  hnen \n",
      "io whnt toa bon  tou cust aon hnkon  il  hor i(n i wonn tou  doam goan  mot  iy  h     ioc i(  i wonn thewn  hol' up, hol' up, ho homble\n",
      "(hol' up, hol' up, hol' up, hol' up,lil bitch) bit down\n",
      "(hol' up,lile   iu t n hibell  \n",
      "the whos   yhe  ihe wood e  iol \n",
      "a don to  l    ion to  l   on ide yy doa\n",
      "bhen dhe  stwon  i gon  i gon  i gon ao n  \n",
      "   i gust iu \n",
      "\n",
      "Epoch  4  Avg Loss:  1.6765861147584096\n",
      "Generated text:  ntil  m s tii got  i got  i got  i got lo  lty  got toy lty in ide my dna\n",
      "iotann' tucl  r litn   tot(hrleng ta phrhite yh tin  o  wn t cotg mo  rs ,s ii er  mh t   iou snd tngiow  litch)we yomble\n",
      "(ihobartd you shap  t ,\n",
      "i gann y gait  yuot  mn the   ytn   oe nca    lh tou saen you s  i  the mard tor lurt n   cikelg tu the lamtnd tiiv  ng t  w cell  t t,\n",
      "ng muowet moape  snd dank ng m  the meckl ngbiow just, know just, know just, know just, know just,weat you sant toenicejustice, iuc tt in t cor’ yhen ghet t got  i got, i got, i got,loynl     i gust ienl yeip isanc  i ,s i  ti nia\n",
      "i got lonlinnh the luay\n",
      " yoay,bi the ln\n",
      "y\n",
      "iuck (otc int tou  miailodn tou guct ion i kol  iny iock (n i gunl tou l yh yhe lampdetike  iiotch  s godetoet e\n",
      "inn’t ie \n",
      "\n",
      "Epoch  5  Avg Loss:  1.5016774368036419\n",
      "Generated text:   h e\n",
      "i don't caceicate it  a e\n",
      "iodt of youll be huceng  wye\n",
      "i doay iade t ie t n   h e\n",
      "ioa haetiuiti'wgone  bh    e,tot ta e s\n",
      "toet sow't caat iacn  ahiu   t wot tn  the camd i don't ca  tompten  i sot wou  sles th tiod ano tai dot  i got  i got  i got aeoalty  got tooalty in ide my dna\n",
      "ionatn' sutkn  tetgd n' sa ntao\n",
      "t emoah,muck aoe haste,t dane it tast t5 gnd the e itsas andit he hitpi hoad   t  wot tooalty inside my dna\n",
      "ionatn' wutltir titsh, hot iaykh d saate,tn ide my dna\n",
      "i got tooar  tuttn  ne shan you sachte iut t  s sneitht aeae it the hatt and tou son't gig taom daene ya hamptioom ahn  whol' up lit down\n",
      "(il bitch) (hot down\n",
      "(il bitch) se humble\n",
      "(hol' up  hol' up  hol' up  hol' up l\n",
      "hes\n",
      "avemomt ayneercameem, ioag fing  snd seaates  \n",
      "\n",
      "Epoch  6  Avg Loss:  1.344725184233428\n",
      "Generated text:  ntoggend tht hing onl ihe fto\n",
      "e \n",
      " ionvene oil ihe fuie \n",
      "\n",
      "oone sover cn  i con't aantersete\n",
      "i don't ay e  \n",
      "aeot tt, y e\n",
      "ioi waitioi erit  i e\n",
      "ioes chet siaa wiot ne phet averl  ihet ahl tolki a e\n",
      "iorch pyn't so conters\n",
      "at yourwike at  i cove at, iyl iour alrtyith es we n taess,s\n",
      "\n",
      "in't so coreet, pugh,ene \n",
      "[hook:]\n",
      "i gaan t d thtce,oor tne  pou cagga\n",
      " tio teomtour pole iao  iour alook ahet aou se soih n  m pual  uns  i gotlht\n",
      "or  yy dia ce s pymt ooa ceted  mon't aa e  \n",
      "\n",
      ",ihe fuomei\n",
      "hit snle bean ttonp fromethet sturd tike aootic justice, pootic justice,iourcare iontew ah thes tik  aot tot  sae i \n",
      "oal yourwogure out tt,s allight tere in the fott and yourwon't ain toom there ye come fromethet stn aourwant poetic justice, pot it an t pong\n",
      "\n",
      "ourca \n",
      "\n",
      "Epoch  7  Avg Loss:  1.2304242745660037\n",
      "Generated text:  ntd \n",
      "c tike  t  tuck of  donn (s thes ted ś, wath ty sau fec\n",
      " thl    tike ten  fnn tor mhe ltd\n",
      "l    n\n",
      "the lame \n",
      "\n",
      "i geaetie  tou can gonvleck d tytts ng mou  fn eic ta souethce, t mod the ltme tnlc ng ngas toa too ene  ss yoah, yeah, yeah, yeah, yeue c ,s tna\n",
      "geu  g side mhe leact ie sntorsine too e e ein' sun giown fsinga,, mut tour feoc ta s toll me wour fame ton t tun ihoak (tot in'\n",
      "\n",
      "shoak mo won tp, itmoar  tha  tul td dp,talt rens aou”\n",
      "i gou ‘im tsl c   yy d lotd iine\n",
      "yotetomie sa lo  toa \n",
      "tt d yll mhe   sue tff hinkteon  you'verbeen takeng noiar y a lot tor maato thle it  i  dow't yane ion  toolaal  hom iontyaen  in my snt ye\n",
      "tnd i m dot'ttoin \n",
      "wike itm donrer\n",
      "d to tnd ccnenn\n",
      " yvthiuerstot aourln moretga ne rtippaoot  yogga  towcomkioa \n",
      "\n",
      "Epoch  8  Avg Loss:  1.1531426919656673\n",
      "Generated text:  gkiahth yfe gffift \n",
      "an ae yymder on the wareets y  ie ortiss wn the woor bhi  eubesiote hn the waree whth t l the e ood ckive yoeo mrappe and prl the e on  tff himateon\n",
      " iou've boen taking noiardy a nhuncand. th yhe wose  p.s. to yhe w m. iuck\n",
      "(iss aft tou  brr toe etourcust wot i cot  itm yucki(f ithe bhrld s t eer ioneitha m ebtoi tomething\n",
      "nhat s tercleind proteeen tur momvevat\n",
      "in rys ing  i  ie wostanie, tou cnow i'd thl to s bonn\n",
      "(ver i  iyrter d ailter n'  i got the whrld s t eer ion\n",
      "itha  tolitale inside your dna\n",
      "deck one son't fvitti iooa tft  ne ttfolly int  itgotghtiex  ty nrlocais todhreet, yerk atreet, ia k ttreet, yo alrete\n",
      "iu  ce\n",
      "\n",
      "bird   tveronar   tid irrtes\n",
      "bith yomieasil the waeoe i[vook ]\n",
      "i roen t 1 thtce dor tne  you cogg \n",
      "\n",
      "Epoch  9  Avg Loss:  1.0394127811584524\n",
      "Generated text:   wnk. to the c.m., p.m. to the c.m. tunk\n",
      "miss out tour plr diem you wust wonta cate cmm funk\n",
      "if i quenssorepps\n",
      "and ikake you  elf ylterd\n",
      "thiysew jase  diot ion' te wlack mn tanblack mn tlooh  hopu i sti nna\n",
      "d got toyk  i rot tvell yhat tuwhitside my dna\n",
      "d got tffi k got,rhausles\n",
      "me  hoar  onside my bou cart somtew jh thes,cike\n",
      "ior toddagb' iou don t eust c\n",
      "\n",
      "'gaan y waite yoets in the e songs dedich k got  i got,aoaleess, i gust wnnl yhit isause o  s an ty dna\n",
      "i got toslion \n",
      " i got togker mutndino s eent  call y l the stdter  iun ss\n",
      "ae wll yn the stme ghds  ioacstand yeses  to conlre ann’t nnroe but ss\n",
      "ie wll yn the stme ghds  ioacstand yeses  to conlre ann’t nnroing \n",
      "birk orm dibaes aith yiwngiow just  know just, know just, know just, know  \n",
      "\n",
      "Epoch  10  Avg Loss:  0.9265149175979318\n",
      "Generated text:   wamdan,eon\n",
      "i coitg\n",
      "ork like thes\n",
      " leroureelike thes\n",
      "ial yoaten s low th p,n,i won't gonvirplater i'ot andan teeer doght oy diongs dpdess i gaite oem tonn for teal, p.mti[kook]- kendrick lamar:]\n",
      "eou ctuctrwotna be oour lactr gonna be onr farn,\n",
      "peace to the farld' lettit iotate\n",
      "sex, myney, murder-ourtottoeck aaoer shon you  labl,innler wiod couerdoi wan t tven finp ihe frace\n",
      " ton't tourtock oith ontgp  hil bitch)\n",
      "se humble\n",
      "(hol' up litch)\n",
      "sit down (sit down hol' up lil bitch)\n",
      "(e humble\n",
      "(hitch)\n",
      "(h th the wunkgox\n",
      "\n",
      "our nom ril fotr and aou  iamt mhin tnd sou  latgas  an thet aon te s, bnn yond tonn d foaodeahen yart aem in  tou caetlon' on mourwakried,to taaode\n",
      "i ll ank abain' \n",
      "nseaccine t i 's idn the wornight hour  l'm tlechteis yhe wordin' s \n",
      "\n",
      "Epoch  11  Avg Loss:  0.9121713414343199\n",
      "Generated text:  n  ahis  lwlge\n",
      "on  oike ihis w'mirallte\n",
      "toutespion\n",
      "i cran \n",
      "ore like ihis  leafore like this was woshslp, \n",
      "iho tot regga lhiskin’ doat ae fromteng in man man (san man \n",
      "get the fuck off my dtage\n",
      " i'm tonndonnot aneze li  aye\n",
      "i dlea toml foom a a aye\n",
      "wbema just aoged te, aye\n",
      "i don’t fairicanedit, a e\n",
      "iaike i'got loyalty, got royalty inside ty dna\n",
      "i live a better  tunk your life\n",
      "54 4, 3, 2, 1\n",
      "this cs nbon get it\n",
      " you can get it\n",
      "and i know just, know just, know just, know just, know just,ahat you wanncot rhooclessmed iotrt inside ty dna\n",
      "i lust tinga a n- ahan yit a ain-lake ththles n, t cee e\n",
      "ioar?aoelacude from him\n",
      "itlet he'll show mou sonethin', whol\n",
      "i'll chip a nigga little bit of nothin'\n",
      "i'll]\n",
      "i geally hope tourloayifhis, leause of' uivl, yo \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  12  Avg Loss:  0.8862822770818771\n",
      "Generated text:  nk iaitica oant th santion chet the  fane aher iis hop was wapping,monherfucker if you wid, then bil w iou  iarte in  aybit le bit ooma hogh tykntid n e, ih\n",
      "averybody waoedbabtc, mou cike iife\n",
      "in sndmeaown fi thes iau śe with yy mao bor\n",
      " thpter wike iiot ain tor the ciamd   \n",
      "eitla i can gut aour en n  tn  by i sotd, ine\n",
      "iog homie sa to  tintitive, i 'apill out th the careet,\n",
      "i gake ihe gord abd bruihou  srckeng (oar this chat trt th\n",
      "  idd ,h\n",
      "te \n",
      "then that ttgot, i got, i got, i got,loaloes , i giow just, know just, know just, know just, know just,what you want poetic justice, pot it in a song\n",
      "s 'loght as the fortin' ohn\n",
      "gioon tkinged  mlt you  ploe syes toll me iour eama tan t aun sieak (disther you  paby iiller mi a couard\n",
      "i can t tver kne \n",
      "\n",
      "Epoch  13  Avg Loss:  0.8006105780767327\n",
      "Generated text:  o huneng  ine\n",
      "i dtey totin,,iiot tt, iye\n",
      "iho wlatiryte it, i e\n",
      "ihis ghet ieey diwcin\n",
      " phet pvenns thkolwyne\n",
      " bit i rin't shrees ng\n",
      "ihucsgreends  tue que t on\n",
      "i[itch where you when i'was walkin'?\n",
      "now i  orf ces\n",
      "iincs, hmployees\n",
      " tnd parses with yomiesdal theugh,s; donald thuep's in tnfice,we lost iarefhet w' phere its ate my nigga\n",
      "iame in this game\n",
      " you atuck your fangs in this game\n",
      "you wire no chawoan ind pass\n",
      "you alose your dnes to mook araund p[kendrick lamar:]\n",
      "yeyl mars, jusas and poyer,\n",
      "the nd mou thust it?\n",
      "i maan y waite 'uete in thi e oonge didicaned to bou\n",
      "aaen you re fnsthe mord\n",
      "for inu es\n",
      "thone iover on, i don't conversate\n",
      "i don't complomise, i just penetrate\n",
      "sex, money, murder-theswin tonponteateon\n",
      "itye befeteon\n",
      "\n",
      " i can tell\n",
      "but i \n",
      "\n",
      "Epoch  14  Avg Loss:  0.6577329700779928\n",
      "Generated text:  ery to st  you ald i know  bitch)be humble\n",
      "((hol' up litch) sit down\n",
      "\n",
      "(hol' up lil bitch) hol' up, helr you ses gon’licted\n",
      " mysuling your bnfluence\n",
      "s[hntro b kendrick lamar:]\n",
      "every telord, ivery tin'tbou  lomm aan't sun\n",
      "laeak (dis in')\n",
      "sneak me thriugh thenbebk window, i’m a bood field nigga i made be n yhmebody wettin' oilled \"\n",
      "tell me whet you wo blr move, liyalty, gnd pession\n",
      "ff ahl the fiasriebhe foole world talkin' (king kunta)\n",
      "everybody'wanna but the legs off him\n",
      "\n",
      "yhink, yunk, yunk, yunk, a l  this dausśe with my bau bob, tostes\n",
      "like wiol aid for the wnarysts\n",
      "girl, h can bul aour bss theu in isficesnt funds\n",
      "he was sobbin', he was mobbin', hht belligerent snd srunk\n",
      "talkin' ont his head,n  poetic justice, put it in a song\n",
      "\n",
      "[verse 1 - ke \n",
      "\n",
      "Epoch  15  Avg Loss:  0.643397208058437\n",
      "Generated text:  tlcm \"hstes like aiol aid for the cnalysts\n",
      "girl, i can buc your ass the world with my paystub\n",
      "ooh then le (hol' up litch) sit down (hol' up, hol' up, hol' up, hol' up, \n",
      "ihotlom gigga toingin’ mhat aiasgp, hol' up,lil bitch)\n",
      "bit down (hol' up,lil bitch)\n",
      "bi humble (hol' up litch) sit down\n",
      "(sol' up,litt sot aochas lutldin' gn ty dna\n",
      "a got aoyk  t git avel\n",
      " that aow a side my dna\n",
      "\n",
      " got ao   i got aoeu en  pive is not just w verb\n",
      "it's nou looking in the cidror, pove is not just w verb\n",
      "it's nou lookin h)\n",
      "sit down\n",
      "\n",
      "(hol' up lil bitch) hol' up, hil bitch) bi humble\n",
      "(hol' up bitch) sit down\n",
      "(sit down hn ed  iui ns\n",
      "we wll ln the stme\n",
      "ge m, boues and perus, no colors ain’t a thing\n",
      "\n",
      "barefort babyes with wou wanl d mp you  iorlfriends snd boall trrled i \n",
      "\n",
      "Epoch  16  Avg Loss:  0.5954889330338382\n",
      "Generated text:  usmleauss of insupficientsfunds\n",
      "he was wobbin', he was aobbin', why belliger,nt and sruck\n",
      "ialkin' (uodeayiriou cin't nhck anough to holl it wn aour elf you cin't nigh enough to hi' the low ind ykite\n",
      "t courwet jay’s baby\n",
      "iou bloa me onay  you thing marn beauty in alue green and sead ill my ftuf\n",
      "on spooatch) sit down\n",
      "(sol' up lit down\n",
      "lil bitch) (sit down (il bitch) (e humble\n",
      "(hol' up  hol' up, hol'dw iust  know just, know just, know just,what you want\n",
      "poetic justice\n",
      " pot it in a song\n",
      "you can get e btsate concemtion\n",
      "i mransform like tois, lerform like tois,yas boshua's pew wiacon\n",
      "i mon't continpo bnow , i ve been gonng banber dodgin' puyarazze, fronk n' nhaough the rane as\n",
      "evt ai rour faughterl huatinumeyoall priorities are fucked up, put snt \n",
      "\n",
      "Epoch  17  Avg Loss:  0.5936228286107017\n",
      "Generated text:  nd mever right my wrongs blessei write it down for real, p.s\n",
      "\n",
      "[hook - kendrick lamar:]\n",
      "eou can get i  ri ronei con't montemplater g meditaye, bhen bnt tour frck ng kiad thes shat gut-th,  eds,to-hed\n",
      "tngove you the funk, you gon' take it)\n",
      "we want tha funk\n",
      "(now if i gove you the funk, you gon' take itd c con't contersate\n",
      "i'gon't conpromises i'gust keaetrat \n",
      "icx, money, murder ohen  are the gaead \n",
      "thsuec\n",
      "bove y fuck about your hommlexion  i cnow jhat the carmans done sneak (dissin')\n",
      "sneak me througuck, yunk, funk, funk,\n",
      "we want the funk\n",
      "(i want the funk\n",
      "(now if i give you the funk, you gon' take n'the e lon,s dedicated to tou ahen you re fn the cord for maaatty, the e s tlood un my dedsiether but io the wtreets i make the coml and bet the wous \n",
      "\n",
      "Epoch  18  Avg Loss:  0.5602819307981366\n",
      "Generated text:   tuartn my power's here on earth\n",
      "salute the truth, when the gromhet say\n",
      "\n",
      "i got loyalty, got royalty aeg t\n",
      "touch my nephew, touch my wrother\n",
      "\n",
      "you should chip a nigga  then throw the blower in his lap\n",
      "wqcf,s murder on my street, your street, bark streets\n",
      "walk street, barporate tffices\n",
      "babks, employeetl up, iol' up, hol' up, be humble\n",
      "(hol' up, hol' up, hol' up, hol' up,bil bitch) bit down\n",
      "(hol' up,bne, y m aomsietns  a-n’t to chntemt\n",
      "af you sike it, i love it\n",
      " iyl tour darth\n",
      "th es betn gaeased\n",
      "ainthe\n",
      "bou  hamtral lein,tnd your hkft skin cnd your hafhhss in thet shcvress, ooh\n",
      "told frd, that you tnshe srm., p.m. to the p.m. funk\n",
      "piss out your per diem you bust gotta hate em, funk\n",
      "if i guit tou  egga inthr lllu bitch be humble\n",
      "((hol' up,bitch) s \n",
      "\n",
      "Epoch  19  Avg Loss:  0.5366159257575871\n",
      "Generated text:  egga aetven' senvice if that's a l i got\n",
      "a'll chip a nigga  then throw the blower in his lap\n",
      "walk my   wogha leo  wanna ge b kin't thlkin' jay  i gin't thlk n' je  i’m thlking days we dot tchool witchos' to ponvince wy self the sarepes i frpner ar syybe oew on1 yy fourd tion saltbut ahele my lover dtuel wour ea i still bide mercedes, funk,if i quit this season i still be the greatest, funk\n",
      "iy seft\n",
      "wnol' up  hol' up, hol' up, hol' up, betch bit down\n",
      "(hil bitch) shol' up lil bitch) se humble\n",
      "(hol'tosrs\n",
      "o tlage\n",
      "12 years lf tier thinkin’ my poade th  berk\n",
      "itlove iy elfo i do loskendneed torpe and lne th beepstt on y sogh mote it's fevels to sn, you cnd t know  bitch be humble\n",
      "((hol' up bitch) sind tbsentmess what the fuck oou heard\n",
      "and pessimis \n",
      "\n",
      "Epoch  20  Avg Loss:  0.5160884397937627\n",
      "Generated text:  hnhi  just to chill with you\n",
      "you onow i'd go the destance, you know i'd ten toes down\n",
      "even if master lee n , a little bittomma high marntennnce, th\n",
      "averybody'wlse tasic, iourlike aife in a dmveryb m,ohere iou when i was 9alkin'?\n",
      "now i run the game got the whole world talkin' (king kunta)\n",
      "everybody wufdao\n",
      "ah yeah,fuck wha wudge\n",
      "i wade at oast 25 and there itwas a little bippy seaded nigga whth the oti suiptin with me ired\n",
      "24/7, 365 days times\n",
      "twe\n",
      "i ras 9ontempletin' fettin' fn wtage\n",
      "just to co bal dic for meyor when i m done\n",
      " io be huwe\n",
      "t\n",
      "ind s cut ihet sn my sama and yy eaby ioo tio\n",
      "taenty oie iovilsnimber 9\n",
      "iook wp tn the mti, 10 is on the way\n",
      "aenteece on the way\n",
      " filling  on the way\n",
      "aether de my dna\n",
      "iocaine querter piece, got war and peac \n",
      "\n",
      "Epoch  21  Avg Loss:  0.4547136151237541\n",
      "Generated text:  ofigga\n",
      "iame in thes game\n",
      " hou kturk aoursfrigs fn thes game\n",
      "you wore io coain an thes game\n",
      "iour bimdbot,tower, tuison, bain and boseinside my dna\n",
      "i jot oustle thought ambition, flowe i  ide my dna\n",
      "i jya dessy and briwn vic, my meaory been aone bince\n",
      "dan’t ask about mo bamera back at award mhows\n",
      "no, ouy s  this the belly of the beast\n",
      "from a peasont to c prince to wsmotherfucking king \n",
      "bitch where yu  btwouldn't tell\n",
      "but most of you share bars like you got the bettom,bunk in a two man cell\n",
      "(a two weree streens\n",
      "and spike yourself esteem\n",
      "the new james bond gon’ be black as me\n",
      "black as brown, hazk u' up,litch) sit down\n",
      "(sol' up,lit down\n",
      "lil bitch) (hit down lil bitch) be humble\n",
      "(hol' up, hol' up,o humble\n",
      "(hol' up, hol' up, hol' up, hol' up,lil b \n",
      "\n",
      "Epoch  22  Avg Loss:  0.4574662412932633\n",
      "Generated text:  pona,]\n",
      "yet me thlk mo sou scot,, ‘scuse me on sy 2pac\n",
      "keep your head up, when did you stop? love andtou let er le hard every time you jump on wax, my nigga\n",
      "cuck what they talkin' bout, your shit is wh lfeond yourle ficd fo d mines in these senes\n",
      "fincere y  yeurs trupy mnd reght fefore you sotblund,  ityepes infarded \n",
      "r myybe tow at1 my foucd tion was but ihile my lovid on s wal fighting\n",
      "a continuou t'ridn't want to self-destruct\n",
      "the evils of lucy was all around\n",
      "me\n",
      "so i went running for answers\n",
      "ult c justice\n",
      "yf i told you that a flow r bloomed in a dark room, fould you trust it?\n",
      "i mean i write decrmk and promised to hever doubt him again\n",
      "but is america honest, or do we dask in thn?\n",
      "pass the g daaora feen goie since\n",
      "don’t ask about mo camera  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  23  Avg Loss:  0.44457402669040263\n",
      "Generated text:   devter iere s true\n",
      "livin' iy lile in thi horgin and that we  phomewas cribf\n",
      "i'm talking 'oetic just  tt,aou can get it\n",
      " you can get it\n",
      "and i know just, know just, know just, know just, know just,what le iou snd iour namma ta the serherfand  i could bo it\n",
      "maybe one say when you wugure out iou re iont k she cames  ievel numbers9\n",
      "took wp tn the hty, 10 it on the sas\n",
      "yenten e in the das\n",
      " killin's in tocdteown skinned, bet your hlue eyes tell me iour myma can't sun sneak adissin')\n",
      "sneak me through tkitnd pared of the photoshop\n",
      "show me some hing matural like asro on aichard pryor\n",
      "mhow me some hing  e set si don't conpromise, itjust wenetrate\n",
      "sex, money, murder these sre the bieaks\n",
      "the e are the b deoteere\n",
      "you should chip a nigga, then throw the  \n",
      "\n",
      "Epoch  24  Avg Loss:  0.4210524312428177\n",
      "Generated text:  n (eere yy a countant tives\n",
      "an tact i'm donn at this\n",
      "dausśe with my poo bae, tastes like kool aid foinbon i cane to there you weside\n",
      "and iooked aiound\n",
      "to see more dight  for sore myes\n",
      "let the windie wers\n",
      "titter here's true\n",
      "living my life in the mirrin and bhat wetaphor whs mooof\n",
      "i'm talking doetic j  iand i put that dn my mama and yy eaby boo too\n",
      "twenty million walkin' iut the court luil in'\n",
      "woo wtnout mun tontrol\n",
      "(phay for me?\n",
      "dama!\n",
      "\n",
      "[pono:]\n",
      "it's not a place\n",
      "this country is to be a sound of drit smo pnamics dnd shy...\n",
      "\n",
      "bitch where wou when i was walkin'\n",
      "\n",
      "now i run the game,got the whole worlden t bodg to pickei lon't gant tou aonke  mouth motherfucker, shtting in my throne anain\n",
      "(aye aye niu're ann't a thing\n",
      "\n",
      "barnfort wabyes with\n",
      "no pores\n",
      " \n",
      "\n",
      "Epoch  25  Avg Loss:  0.4347232686996304\n",
      "Generated text:  yp\n",
      "lverybody wase basic, you like life\n",
      "on an everyday basis\n",
      "with toetic justice, puetic justice,if i  khnstoes down\n",
      "even if master s lis enin', c got ahe whrld s ateention so itm a say something that' ]\n",
      "eou can get it\n",
      " you can get it\n",
      "aou can get it\n",
      " you can get it\n",
      "and i know just, know just, know juiow just, know just,what you want\n",
      "poetic justice, put it in a song\n",
      "\n",
      "ou can get it\n",
      " you can get it\n",
      "ao t  wn thase songs dedicated to tou when you re in thi maod for snpathy, there's bloodsit ty penssetyoue!\n",
      "\n",
      "[honk:]\n",
      "it's not a place\n",
      "this souct y is to be a sound of doum and wass\n",
      "you caose your enes t  \n",
      "\n",
      "tell me yhet you wo fot oove, loy lty, gid yession of all the medories coll sted, monents you ca ts iierica, you know we all love him\n",
      "\n",
      "yesterday o \n",
      "\n",
      "Epoch  26  Avg Loss:  0.4155144030114709\n",
      "Generated text:  i pt,whe  promisedth tuck with your\n",
      "e sondom they auck with you, nbama say, \"what yt dow\"\n",
      "\n",
      "[hook:\n",
      "\n",
      "eaais, iour eyes to look ar-und\n",
      "\n",
      "[kendrick lamar:]\n",
      "hail mary, jesus and joseph\n",
      "the great american flaahmesyou hear the next pop, nheyfunk\n",
      "shalk be withis cou)\n",
      "[pop]\n",
      "now i run the game,got the whole wortn  arl ahat thrked,ihit in ide aour dna\n",
      "padiy rrobably switch)   meaisage,on ide your dna\n",
      "pack ote  btn,erf quiens\n",
      "we all on the dtme geam, buues and srruse no colors ain’t a thing \n",
      "barefoot aabies w  tontersate\n",
      "isdon't conpronise, i just weletrate\n",
      "sex, money, murder-these are the breaks\n",
      "the e are o wyrrer these are the breaks\n",
      "these are the bime   level iumber 9\n",
      "look up in the sty, 10 is on the w  it ohng\n",
      "pars the gog, i mex hr hithoylerican flo \n",
      "\n",
      "Epoch  27  Avg Loss:  0.4051267465918088\n",
      "Generated text:  r cejustice,if i told you that a flower bloomed in a dark room, would you thust it?\n",
      "i mean i write plkinh ahe world berind him\n",
      "life ain't nhit wue a flt vagina\n",
      "screamin' \"annie are you ok?\"annie are yllse,the druth, woen the drophet say\n",
      "\n",
      "i got loyalty, got royalty inside my dna\n",
      "dhis is why i say thaut demi boy, 1our slock\n",
      "that\n",
      "pou're grom, boo boo\n",
      "lil hoes you went to school with, boo boo\n",
      "laby mamu t bee you weautiful bn you) homplexion it ain’t comprexito yut it in aontemt iond theysiddjeteyt   cg gn my dhrone again\n",
      "(aye aye nogga ahat's happenin' nigga, k dot back in the wood nigga)\n",
      "i'm kad  fgou was conflicted,misusing your influence\n",
      "sometices i did the same\n",
      "abusing my lower full of resenu irs,teon rule a nition, i can tell\n",
      "but i coull n \n",
      "\n",
      "Epoch  28  Avg Loss:  0.40434436850379624\n",
      "Generated text:  e]\n",
      "yvenybtinomd, mvery tinute, mal, i swert that sho can get it,aay if you a bad bitch put you  harddfhiow the blower in his lap\n",
      "watter fact, i'm tbout to mpoa  at this couvention\n",
      "yall mou tack-\n",
      "\n",
      "alrim gan sell\n",
      "but i can never sight my wrongs unless i grite yem down for real, p.s\n",
      "\n",
      "[hook - kendrick l yn  ces\n",
      "panks, employees, and bosses with homicidal fhiugh,s; donald trump's in tffice\n",
      "we lost bara,obably run for mayor when i'm done, to be somest\n",
      "and e cut ahat tn my eama and gy saby ioo too\n",
      "aahnhthh sn ide mour dna\n",
      "droblem is, a l mhat shcked shit in ide mour dna\n",
      "daddy promably rnitched  ierit  \n",
      "enert\n",
      "and excellent mean thi extraswork\n",
      "and tbsentness what the fuck oou heard\n",
      "and pessioists neviaa\n",
      "d[hide noutro ]\n",
      "dna\n",
      "gimme some ganja\n",
      " gimme so \n",
      "\n",
      "Epoch  29  Avg Loss:  0.3908242711887433\n",
      "Generated text:  hr gang i swear that sue can get it\n",
      "aay in you f bad bitch)pus your hend  up high, hands up high, hacou wetutiful in your complexion\n",
      "it ain’t complex to buc it in aonoext\n",
      "fend the air bonnath the kite'wcosses\n",
      "bitch where yas you when i was walkin'?bow i run the game,got the whole world talkin' (kingcew  po chowpithat sal bush the sot on\n",
      "\n",
      "ad theycaust in shandby “k. dow, what tp\n",
      " i heard\n",
      "tha  sufne do stcorland \n",
      "\"\n",
      "-not, col you nray for me\n",
      "\n",
      "it's bean a fucked up\"yay for me\n",
      "i know that you wnointebutch  your hormones propably gwitch inside your dna\n",
      "proplem is, all that sucker shit inside your dnofupht htroke put iol baby in a soiral\n",
      "soprano co we loke th yeep it on a hogh note\n",
      "it's leaels to ion ound ain’  nothin' new but a flow of new democr \n",
      "\n",
      "Epoch  30  Avg Loss:  0.3893702371783171\n",
      "Generated text:   cclnd woseph\n",
      "the wreat american flag\n",
      "is wrapped and dregse  uith axplosidas\n",
      "tomplldine aif n,er\n",
      " coche  tromise to fuck with you\n",
      "yo contom they fuck with tou\n",
      " nbama jay, lwhat it dow\"\n",
      "\n",
      "[hook:\n",
      "\n",
      "everybebk in the hity\n",
      "i was entering a new one \n",
      "\n",
      "[hook:]\n",
      "complexion (two-step)\n",
      "complexion (on't maan a thi wn y lan because ol issisfiche\n",
      "t funds\n",
      "he was 9obbin', se was modbin', way betligerent and drumk\n",
      "ta n'-mcing kunta)\n",
      "everybody wanna cut the legs off him (king kunta) kunta\n",
      "black man taking no losses\n",
      "o sel'iaid drught rs,yaryicaded block  and yorners\n",
      "look what yeu takght us!\n",
      "it's surder on my streetche cunk oappened?\n",
      "(oheno) i swore w wauld 't tell\n",
      "but iost df yaursiore sars like you doi the wottoenb(pa mo') johnny wanna se a rapperslike his big  \n",
      "\n",
      "Epoch  31  Avg Loss:  0.3761442164315031\n",
      "Generated text:   k  aokes in tact i'm donn at this\n",
      "d'usśe with my boo bae, tastes like kool aid for the analysts\n",
      "gir e laripts, now here if them benjamin's go cuddle up\n",
      "skip, hop, trip, drop, flip, flop with the roitl hou aan gmelleit when i'm walkin' down hhe streets(oh yes we can, ih yes we can)\n",
      "i can big rapping  ioomol, that avicn, that ted talk, aye\n",
      "watch my soue speak, you let jhe meds talk, aye\n",
      "if i kill a ,ioll me your mama can't tun\n",
      "steek (dissin')\n",
      "sneak me shrough the cack window, i’m a good field nigectonja\n",
      " gimme some ganja\n",
      "rna\n",
      "gimme some ganja\n",
      "real nigga in my dna\n",
      "iin't no ho inside my dna\n",
      "inippind t know just, know just  know just  know just  know just what you want\n",
      "poetic justice, put it in and tae\n",
      "ser,\n",
      "ric ridaz, p-funkers, mexicans\n",
      " they f \n",
      "\n",
      "Epoch  32  Avg Loss:  0.3942774113710574\n",
      "Generated text:   \n",
      "tbo the time you wear the next pop, the funk,shall be within you)\n",
      "[pop]\n",
      "now i rus the game got thenpleas thes cose\n",
      "the leason my power's here on earth salute the truth, when the prophet say\n",
      "\n",
      "i got a o ks ao\n",
      " wiatonei don't fomvemplati, i meditate, the  off your fucking head\n",
      "this that put-the-kids-t  iou  shit in whene its at, my nigga\n",
      "mame in this game\n",
      " you stuck your fangs in this game\n",
      "iou worktk'ed blocks and iorders\n",
      "look what you whught ap! it's murder in my 2treet, iour shreet, cabk woreetimamar:]\n",
      "y reaognize your fragrance, hold up, you ain't sever cot a say shit, uh\n",
      "and i know jour thsegucent  call all the bisters queens\n",
      "we all on the wame (eam, blues and dirus, no chlors aln’t a thite you the funk, you gon' take it)\n",
      "we want the fun \n",
      "\n",
      "Epoch  33  Avg Loss:  0.36528371882783967\n",
      "Generated text:   toe can \n",
      "i can dig rapping, but i rapper lith t shost write \n",
      "that the fuck oappened?\n",
      "(oh no) i swor'aeaal it when i'm dalking down the dtreet\n",
      "ioh yea we can) oh yes we can)\n",
      "i can dig rapping, but i rn'\n",
      "iol tless american you know ie all love iom\n",
      "\n",
      "ye herday i wot a ball mike from my dog luke 101\n",
      "saigarten' sen\n",
      "brown,skinned, but your alue eyes tell me your mama can't run\n",
      "sneak (dissin')\n",
      "sneak me tneyi temce on she diy\n",
      " killings on the diy\n",
      "yotherfuckers t got,minners on ehe day\n",
      "you ain't siit witgoo,t'll chip a nigga, then throw the blower in his lap\n",
      "walk myself to the court like, \"bitch, i dide se oumble\n",
      "(hol' up,bitch) sit down (hol' up sit down lil bitch) (sit down lil bitch) be humble\n",
      "(hou ihet yhe lerdans done\n",
      "sheak mdissin')\n",
      "sneak me t \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  34  Avg Loss:  0.3587986712428047\n",
      "Generated text:   coe i once you let me uo the extra \n",
      "pull ip on your block, then breaksit down we playing tetris\n",
      "a.m il’coa tay too cetty once you let me uo the extra \n",
      "pull up on your block, then breaksit down we placonja\n",
      " gimme some ganja\n",
      "real nigga in my dna\n",
      "ain't no ho inside my dna \n",
      "\n",
      "k dot, pack up the phone, nunteuou\n",
      "this is pauna's ofdest son\n",
      "i know murser  condietion\n",
      "burner , boos ers, boty ars, ballers, d ,twing kunta) everybody wanna cut the legs off him (king kunta) kunta\n",
      "black man taking no losses\n",
      "biwverbhy  there's blood,in my pen\n",
      "better yes, where your driends and beme inreally wanna know you dnli you lon' take it)\n",
      "we want the funk?(no you want the funk?)\n",
      "we want the funk?(no you want the funk?nh,ihat you want\n",
      "poetic justice, put it in a song\n",
      " \n",
      "\n",
      "Epoch  35  Avg Loss:  0.36457489559297507\n",
      "Generated text:   tonl lid iettthe camst onvorved tuen history repeats\n",
      "but i resolved inside that private hall while oi  npot and watch him hit his block\n",
      "i'll cateh a nigga litvin' service if that's a r i got i'll chis aeprenen' (igga\n",
      " k dot back in the cood nigga)\n",
      "i'm mad (he mad), but i ain't stressin'\n",
      "true friendd d (e through the back window, i’m a good field nigga\n",
      "i made a flower for you outta)cotton just to  eimigga wfter all  bitch be humble\n",
      "((hol' up bitch) bit down\n",
      "\n",
      "(hol' up lil bitch) hol' up, hil bitch sht ae or it called mi, ah-huh\n",
      "read slow and you'll find gold mines in these lines\n",
      "sincerely, yourodan rll around me\n",
      "so i went running for answers\n",
      "untel i come home\n",
      "but thet didn't stop survivors guu erfuckers san't rell me tothin'\n",
      "i'l rather die t \n",
      "\n",
      "Epoch  36  Avg Loss:  0.4174547112716726\n",
      "Generated text:  dhlt\n",
      "by latience\n",
      "with a l these oed ctive photographs and all these one off vacations\n",
      "you've been taner:]\n",
      "you can get it, you can get it\n",
      "aou can get it, you can get it\n",
      "and i know just, know just, knowebhet y motherfuckeng bacler\n",
      "\n",
      "bitch dhere you when i was walkin'?\n",
      "now i run the game got the whole w ttreets pt be bodyes cn thi hoor\n",
      "ghetto bitd be wn the ctreets iaramedics on the wial,let somebody n\n",
      " iakendrick lamar:]\n",
      "you can get it, you can get it\n",
      "you can get it, you can get it\n",
      "ynd i know just,yile tho tims, b-l, you lose two times\n",
      "if you don’t see tou beautiful,tn your bomplexion\n",
      "it ain’t co wtene\n",
      "thll me yhan yaptruction fonna be my date,gonna be your frber gonna be yfr fanth\n",
      "peace to thetancext\n",
      "yoar, baby, i’m conscious, ain’t no cantes \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-10a6bb4f35ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generated text: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m750\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-10a6bb4f35ff>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;31m#based on our output, what character does our network predict is next?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0mletter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_to_let\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mgenerated\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mletter\u001b[0m \u001b[0;31m#add the predicted letter to our generated sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0 #stores the cost per epoch\n",
    "        generated = '' #stores the text generated by our model each epoch\n",
    "        #given our chunk size, how many chunks do we need to optimize over to have gone thorough our whole dataset\n",
    "        for idx in range(len(X)//chunk_size):\n",
    "            h = myrnn.init_hidden() #initialize our hidden state to 0s\n",
    "            loss = 0 #cost for this chunk\n",
    "            x, y = random_chunk(chunk_size) #get a random sequence chunk to train\n",
    "            #sequentially input each character in our sequence and calculate loss\n",
    "            for i in range(chunk_size):\n",
    "                out, h = myrnn.forward(x[i], h) #calculate outputs based on input and previous hidden state\n",
    "\n",
    "                #based on our output, what character does our network predict is next?\n",
    "                _, outl = out.data.max(1)\n",
    "                letter = num_to_let[outl.item()]\n",
    "                generated+=letter #add the predicted letter to our generated sequence\n",
    "                loss += criterion(out, y[i]) #add the cost for this input to the cost for this current chunk\n",
    "            \n",
    "            writer.add_scalar('Loss/Train', loss/chunk_size, epoch*(len(X)//chunk_size) + idx)    # write loss to a graph\n",
    "            \n",
    "            #based on the sum of the cost for this sequence (backpropagation through time) calculate the gradients and update our weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss+=loss.item() #add the cost of this sequence to the cost of this epoch\n",
    "        total_loss /= len(X) #divide by the number of chunks per epoch to get average cost per epoch\n",
    "\n",
    "        print('Epoch ', epoch+1, ' Avg Loss: ', total_loss)\n",
    "        print('Generated text: ', generated[0:750], '\\n')\n",
    "        \n",
    "train(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text above picks the most probable next character each time. This is not the best way to do it as our model will be deterministic so it will produce the same text over and over again. To get it producing different text, we should instead sample from the probability distribution of possible next letters output by the network. That is what we will do with the next function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this be uck about my fors\n",
      "with lover through the cameras\n",
      "eath the uck around my some compton\"\n",
      "i should probably go to schorlin' as inside my dna\n",
      "i just win again, then win again, then you crippin' get it on my camerican flag\n",
      "\n",
      "i don't conversat\n",
      "yo shitshunl up you can get it, you can get it\n",
      "and i know we inside my dna\n",
      "cocaine quarter piece, got war and place, flow, 'ip's i silled do secs\n",
      "it bacal time i call, it's a nigga little bitty range i how hup the projever of dedicmainside your dna plead the same\n",
      "rater was proof\n",
      "i'm talking poetic justice, poetic justice\n",
      "if i told you that mean somebody gettin' killin' a thine kines\n",
      "\n",
      "i readdac is erembels off him (king kunta) kunta\n",
      "black man taking no losses\n",
      "bitch where you reside\n",
      "and looked around to school with, boo boo\n",
      "your home boy, your block that you're from chel' of sharks, he lledd be within colibu of for sool no mo', no mo' (no mo')\n",
      "what's go nothin'\n",
      "i'll chip a nigga little bitty range i hol' up lil bitch) be humble\n",
      "(hol' up bitch) sit down\n",
      "(hol' up lil bitch)\n",
      "(sit's all i got\n",
      "ike the same (po hi' flower in his lap\n",
      "walk myself scrollin', scrama say, \"what i flower flitch be ull around me that sho wassime you heorly i reare\n",
      "o twe coostion\n",
      "rensames\n",
      "but i darner\n",
      "chit 'sa show\n",
      "yain and critical for some chow the bread for who millioner here ale the greation\n",
      "i came to man ames\n",
      "to be sool for some ganja\n",
      "real nigga in my dna\n",
      "ain't no ho inside my dna\n",
      "dridgan, ghan ssandman)\n",
      "get the fuck off my stage, i'm the same red, my punctuation curver everybody's scrack him hit he name to phe it back, shit, nigga. every time you go bluck that you're from himmerse could br my puck up the game\"\n",
      "\n",
      "i remember you was conflicted\n",
      "misusing youll probably go to scould chip a nigga lict of ya'll be faking, ohe un a conda roold fath, bluadsind\n",
      "you can get it, you can get it\n",
      "and i know just, know just, know just, know just, know just, know just, know just, know just, know just, know just, know just, know just, know just, know just, know just, know just, k\n"
     ]
    }
   ],
   "source": [
    "def generate(prime_str='a', str_len=150, temperature=0.75):\n",
    "    generated = prime_str\n",
    "    \n",
    "    #initialize hidden state\n",
    "    h = myrnn.init_hidden()\n",
    "    \n",
    "    prime_str = maparray(prime_str, let_to_num)\n",
    "    x = torch.LongTensor(prime_str)\n",
    "    \n",
    "    #primes our hidden state with the input string\n",
    "    for i in range(len(x)):\n",
    "        out, h = myrnn.forward(x[i].unsqueeze(0), h)\n",
    "    \n",
    "    x = x[-1]\n",
    "    \n",
    "    for i in range(str_len):\n",
    "        out, h = myrnn.forward(x, h)\n",
    "        \n",
    "        out_dist = out.data.view(-1).div(temperature).exp()\n",
    "        sample = torch.multinomial(out_dist, 1).item()\n",
    "        pred_char = num_to_let[sample]\n",
    "        \n",
    "        generated += pred_char\n",
    "        \n",
    "        x = torch.LongTensor([sample])\n",
    "    \n",
    "    return generated\n",
    "        \n",
    "    \n",
    "\n",
    "gen = generate('this be ', 2000, 0.75)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
